# FindersKeepers v2 - Personal AI Agent Knowledge Hub
# Free, Local, Docker-Based Architecture (July 2025)

# version: '3.8'  # Deprecated in newer Docker Compose

services:
  # ========================================
  # WORKFLOW AUTOMATION
  # ========================================
  n8n:
    image: n8nio/n8n:latest
    container_name: fk2_n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=admin
      - N8N_BASIC_AUTH_PASSWORD=finderskeepers2025
      - WEBHOOK_URL=http://localhost:5678/
      - GENERIC_TIMEZONE=America/New_York
      - N8N_JWT_SECRET=${N8N_JWT_SECRET:-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI4MzY1MDU0ZS05YjY0LTRiY2ItOTFhZS0yMWRjNTc3NzRjMjUiLCJpc3MiOiJuOG4iLCJhdWQiOiJwdWJsaWMtYXBpIiwiaWF0IjoxNzQyNjU4ODc0fQ.P4R2cmeV9uwsUH3Dohf8kvhEXMIoAu_3MwElSvvyiH8}
      # GPU environment variables
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - n8n_data:/home/node/.n8n
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - postgres
      - redis
    networks:
      - finderskeepers_network

  # ========================================
  # API BACKEND
  # ========================================
  fastapi:
    build:
      context: ./services/diary-api
      dockerfile: Dockerfile
      args:
        DOCKER_USERNAME: ${DOCKER_USERNAME:-}
        DOCKER_TOKEN: ${DOCKER_TOKEN:-}
      x-bake:
        platforms:
          - linux/amd64
          - linux/arm64
    container_name: fk2_fastapi
    restart: unless-stopped
    ports:
      - "8000:80"
    environment:
      - BUILDX_BUILDER=${BUILDX_BUILDER:-}
      - PYTHONPATH=/app
      - LOG_LEVEL=info
      # Database connections
      - POSTGRES_URL=postgresql://finderskeepers:fk2025secure@postgres:5432/finderskeepers_v2
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=fk2025neo4j
      - REDIS_URL=redis://redis:6379
      - QDRANT_URL=http://qdrant:6333
      # Local LLM Configuration
      - OLLAMA_URL=http://ollama:11434
      - USE_LOCAL_LLM=true
      - EMBEDDING_MODEL=mxbai-embed-large
      - CHAT_MODEL=llama3.2:3b
      # AI API Keys (fallback if local unavailable)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker socket for container monitoring
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - postgres
      - neo4j
      - redis
      - qdrant
      - ollama
    networks:
      - finderskeepers_network

  # ========================================
  # VECTOR DATABASES
  # ========================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: fk2_postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=finderskeepers_v2
      - POSTGRES_USER=finderskeepers
      - POSTGRES_PASSWORD=fk2025secure
      - PGDATA=/var/lib/postgresql/data/pgdata
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - postgres_data:/var/lib/postgresql/data
    command: >
      postgres -c shared_preload_libraries=vector 
               -c log_destination=stderr 
               -c log_statement=all 
               -c log_min_duration_statement=0
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - finderskeepers_network

  qdrant:
    image: qdrant/qdrant:latest
    container_name: fk2_qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # GPU acceleration for vector operations
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=32
      - QDRANT__SERVICE__MAX_WORKERS=8
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=8
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - qdrant_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - finderskeepers_network

  # ========================================
  # KNOWLEDGE GRAPH
  # ========================================
  neo4j:
    image: neo4j:5.18-community
    container_name: fk2_neo4j
    restart: unless-stopped
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/fk2025neo4j
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - finderskeepers_network

  # ========================================
  # CACHING & SESSION MANAGEMENT
  # ========================================
  redis:
    image: redis:7-alpine
    container_name: fk2_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server /etc/redis/redis.conf
    volumes:
      - redis_data:/data
      - ./config/redis/redis.conf:/etc/redis/redis.conf:ro
    networks:
      - finderskeepers_network

  # ========================================
  # LOCAL LLM INFERENCE (GPU-ACCELERATED)
  # ========================================
  ollama:
    image: ollama/ollama:latest
    container_name: fk2_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_GPU_LAYERS=999
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
    volumes:
      - ollama_data:/root/.ollama
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      - finderskeepers_network

  # ========================================
  # REACT FRONTEND (Vite Development Server)
  # ========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: fk2_frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000
      - VITE_NEO4J_URI=bolt://localhost:7687
      - VITE_NEO4J_USER=neo4j
      - VITE_NEO4J_PASSWORD=fk2025neo4j
      - VITE_QDRANT_URL=http://localhost:6333
      - CHOKIDAR_USEPOLLING=true  # For hot reload in Docker
      - WATCHPACK_POLLING=true
    volumes:
      - ./frontend:/app
      - /app/node_modules  # Anonymous volume for node_modules
      - /var/run/docker.sock:/var/run/docker.sock:ro  # Docker socket for file operations
    depends_on:
      - fastapi
      - neo4j
    networks:
      - finderskeepers_network

  # ========================================
  # MCP N8N - Model Context Protocol Integration
  # ========================================
  mcp-n8n:
    image: ghcr.io/czlonkowski/n8n-mcp:latest
    container_name: fk2_mcp_n8n
    restart: unless-stopped
    ports:
      - "3002:3000"
    environment:
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    # MCP server runs on stdio transport, not HTTP - removed healthcheck
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - fastapi
    networks:
      - finderskeepers_network

  # ========================================
  # CHAT INTERFACE (Future Enhancement)
  # ========================================
  # chat-ui:
  #   image: python:3.12-slim
  #   container_name: fk2_chat_ui
  #   restart: unless-stopped
  #   ports:
  #     - "8501:8501"
  #   environment:
  #     - STREAMLIT_SERVER_PORT=8501
  #     - STREAMLIT_SERVER_ADDRESS=0.0.0.0
  #   volumes:
  #     - ./services/chat-interface:/app
  #   working_dir: /app
  #   command: >
  #     bash -c "pip install streamlit && streamlit run app.py"
  #   depends_on:
  #     - fastapi
  #   networks:
  #     - finderskeepers_network

networks:
  finderskeepers_network:
    driver: bridge
    name: finderskeepers_v2_network

volumes:
  # Named volumes for data persistence
  postgres_data:
  neo4j_data:
  neo4j_logs:
  qdrant_data:
  redis_data:
  n8n_data:
  ollama_data:

# ========================================
# USAGE INSTRUCTIONS
# ========================================
# 1. Start all services:
#    docker-compose up -d
#
# 2. Access points:
#    - n8n Workflows: http://localhost:5678 (admin/finderskeepers2025)
#    - FastAPI Docs: http://localhost:8000/docs
#    - Neo4j Browser: http://localhost:7474 (neo4j/fk2025neo4j)
#    - Qdrant API: http://localhost:6333
#
# 3. Initial setup:
#    - Create database schemas
#    - Import existing FindersKeepers data
#    - Configure n8n workflows for agent logging
#
# 4. Environment variables needed:
#    - OPENAI_API_KEY
#    - GOOGLE_API_KEY  
#    - ANTHROPIC_API_KEY
#    - N8N_JWT_SECRET (provided above)