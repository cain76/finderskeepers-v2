# FindersKeepers v2 - Personal AI Agent Knowledge Hub for bitcain
# Updated Docker Compose with latest n8n and security improvements
# Enhanced with optimized Ollama configuration for RTX 2080 Ti

services:
  # ========================================
  # WORKFLOW AUTOMATION - UPDATED n8n (OPTIONAL - Bypassed for MCP)
  # n8n is now optional since MCP server uses direct FastAPI integration
  # Keep this running if you need it for other workflows
  # ========================================
  n8n:
    image: docker.n8n.io/n8nio/n8n:latest  # Official n8n registry with latest stable version
    container_name: fk2_n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      # Basic Authentication
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-jeremy.cn.davis@gmail.com}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-Toolman7424!}
      
      # Network Configuration
      - WEBHOOK_URL=http://localhost:5678/
      - N8N_PROTOCOL=http
      - N8N_HOST=localhost
      - N8N_PORT=5678
      
      # Security & Performance
      - GENERIC_TIMEZONE=America/Chicago
      - N8N_JWT_SECRET=${N8N_JWT_SECRET:-eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJzdWIiOiI4MzY1MDU0ZS05YjY0LTRiY2ItOTFhZS0yMWRjNTc3NzRjMjUiLCJpc3MiOiJuOG4iLCJhdWQiOiJwdWJsaWMtYXBpIiwiaWF0IjoxNzQyNjU4ODc0fQ.P4R2cmeV9uwsUH3Dohf8kvhEXMIoAu_3MwElSvvyiH8}
      
      # Fix deprecation warnings
      - N8N_RUNNERS_ENABLED=true  # Enable task runners (required for future versions)
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true  # Enforce secure file permissions
      
      # Database Configuration (PostgreSQL)
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=finderskeepers_v2
      - DB_POSTGRESDB_USER=finderskeepers
      - DB_POSTGRESDB_PASSWORD=fk2025secure
      
      # Enhanced Features
      - N8N_TEMPLATES_ENABLED=true
      - N8N_DIAGNOSTICS_ENABLED=false  # Disable telemetry for privacy
      - N8N_VERSION_NOTIFICATIONS_ENABLED=true
      - N8N_COMMUNITY_PACKAGES_ENABLED=true
      
      # GPU environment variables for bitcain's RTX 2080 Ti
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
      
      # Workflow execution settings - REGULAR MODE FOR SINGLE INSTANCE
      - EXECUTIONS_MODE=regular
      # Queue settings disabled for regular mode
      # - QUEUE_BULL_REDIS_HOST=redis
      # - QUEUE_BULL_REDIS_PORT=6379
      # - OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS=true
      - N8N_RUNNERS_ENABLED=true
      - N8N_RUNNERS_MODE=internal
      
      # Logging
      - N8N_LOG_LEVEL=debug
      - N8N_LOG_OUTPUT=console
      
    volumes:
      - n8n_data:/home/node/.n8n
      # Custom node modules if needed
      - ./n8n/custom-nodes:/home/node/.n8n/custom
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      shared-network:
        aliases:
          - n8n

  # ========================================
  # API BACKEND - Enhanced for bitcain with Ollama 7B
  # NOW HANDLES DIRECT MCP INTEGRATION - Bypassing n8n for reliability
  # ========================================
  fastapi:
    build:
      context: ./services/diary-api
      dockerfile: Dockerfile
      args:
        DOCKER_USERNAME: ${DOCKER_USERNAME:-bitcainnet}
        DOCKER_TOKEN: ${DOCKER_TOKEN:-}
      x-bake:
        platforms:
          - linux/amd64
          - linux/arm64
    container_name: fk2_fastapi
    restart: unless-stopped
    ports:
      - "8000:80"
    environment:
      - BUILDX_BUILDER=${BUILDX_BUILDER:-}
      - PYTHONPATH=/app
      - LOG_LEVEL=info
      
      # Database connections for bitcain
      - POSTGRES_URL=postgresql://finderskeepers:fk2025secure@postgres:5432/finderskeepers_v2
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=fk2025neo4j
      - REDIS_URL=redis://redis:6379
      - QDRANT_URL=http://qdrant:6333
      
      # Local LLM Configuration optimized for RTX 2080 Ti - UPDATED TO 7B MODEL
      - OLLAMA_URL=http://ollama:11434
      - USE_LOCAL_LLM=true
      - EMBEDDING_MODEL=mxbai-embed-large  # DO NOT CHANGE - 1024 dimensions required
      - CHAT_MODEL=llama3:8b  # UPGRADED from 3b to 8b for better performance
      
      # AI API Keys (fallback if local unavailable)
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY}
      
      # Enhanced n8n integration
      - N8N_WEBHOOK_URL=http://n8n:5678
      - N8N_API_URL=http://n8n:5678/api/v1
      
    volumes:
      - ./services/diary-api:/app
      - /var/run/docker.sock:/var/run/docker.sock:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - postgres
      - neo4j
      - redis
      - qdrant
      - ollama
      - n8n
    networks:
      shared-network:
        aliases:
          - fastapi

  # ========================================
  # VECTOR DATABASES - Enhanced for bitcain
  # ========================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: fk2_postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=finderskeepers_v2
      - POSTGRES_USER=finderskeepers
      - POSTGRES_PASSWORD=fk2025secure
      - PGDATA=/var/lib/postgresql/data/pgdata
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U finderskeepers -d finderskeepers_v2']
      interval: 5s
      timeout: 5s
      retries: 10
    command: >
      postgres -c shared_preload_libraries=vector 
               -c log_destination=stderr 
               -c log_statement=all 
               -c log_min_duration_statement=0
               -c max_connections=200
               -c shared_buffers=256MB
               -c effective_cache_size=1GB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - postgres

  qdrant:
    image: qdrant/qdrant:latest
    container_name: fk2_qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # GPU acceleration for vector operations
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=32
      - QDRANT__SERVICE__MAX_WORKERS=8
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=8
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - qdrant_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - qdrant

  # ========================================
  # KNOWLEDGE GRAPH - Enhanced for bitcain
  # ========================================
  neo4j:
    image: neo4j:5.18-community
    container_name: fk2_neo4j
    restart: unless-stopped
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/fk2025neo4j
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - neo4j

  # ========================================
  # CACHING & SESSION MANAGEMENT
  # ========================================
  redis:
    image: redis:7-alpine
    container_name: fk2_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server /etc/redis/redis.conf
    volumes:
      - redis_data:/data
      - ./config/redis/redis.conf:/etc/redis/redis.conf:ro
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      shared-network:
        aliases:
          - redis

  # ========================================
  # LOCAL LLM INFERENCE - ENHANCED CONFIGURATION
  # Optimized for RTX 2080 Ti (11GB VRAM) with July 2025 recommendations
  # ========================================
  ollama:
    image: ollama/ollama:latest
    container_name: fk2_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      # GPU Configuration
      - OLLAMA_GPU_LAYERS=999
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      
      # Optimized for RTX 2080 Ti (11GB VRAM)
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_QUEUE=512
      
      # PERFORMANCE OPTIMIZATIONS (July 2025)
      - OLLAMA_FLASH_ATTENTION=1         # Enable flash attention for faster inference
      - OLLAMA_GPU_MEMORY_FRACTION=0.95  # Use 95% of available VRAM
      - OLLAMA_PARALLEL_CONTEXT=2048     # Larger context window for better understanding
      - OLLAMA_BATCH_SIZE=512            # Larger batch for better throughput
      
      # Model Configuration
      - OLLAMA_DEFAULT_CHAT_MODEL=llama3:8b       # Default chat model
      - OLLAMA_DEFAULT_EMBED_MODEL=mxbai-embed-large  # Default embedding model
      
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/ollama-init.sh:/init.sh:ro  # Model initialization script
    # Run initialization script on first start
    entrypoint: ["/bin/sh", "-c", "if [ ! -f /root/.ollama/.initialized ]; then /init.sh && touch /root/.ollama/.initialized; fi && ollama serve"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - ollama

  # ========================================
  # REACT FRONTEND - Enhanced for bitcain
  # ========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: fk2_frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000
      - VITE_NEO4J_URI=bolt://localhost:7687
      - VITE_NEO4J_USER=neo4j
      - VITE_NEO4J_PASSWORD=fk2025neo4j
      - VITE_QDRANT_URL=http://localhost:6333
      - VITE_N8N_URL=http://localhost:5678
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /var/run/docker.sock:/var/run/docker.sock:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - fastapi
      - neo4j
      - n8n
    networks:
      shared-network:
        aliases:
          - frontend

  # ========================================
  # WEB SCRAPING SERVICE
  # ========================================
  webscraper:
    build:
      context: ./services/crawl4ai-service
      dockerfile: Dockerfile
    container_name: fk2_webscraper
    restart: unless-stopped
    ports:
      - "8001:8001"
    environment:
      - PYTHONPATH=/app
    depends_on:
      - fastapi
    networks:
      shared-network:
        aliases:
          - webscraper

networks:
  shared-network:
    name: shared-network
    external: true

volumes:
  # Named volumes for data persistence
  postgres_data:
  neo4j_data:
  neo4j_logs:
  qdrant_data:
  redis_data:
  n8n_data:
  ollama_data:

# ========================================
# bitcain USAGE INSTRUCTIONS - UPDATED FOR DIRECT API INTEGRATION
# ========================================
# 1. Start all services:
#    docker compose up -d
#
# 2. Access points:
#    - FastAPI Docs: http://localhost:8000/docs (NOW WITH MCP ENDPOINTS)
#    - MCP Health: http://localhost:8000/api/mcp/health
#    - n8n Workflows: http://localhost:5678 (OPTIONAL - jeremy.cn.davis@gmail.com/Toolman7424!)
#    - Neo4j Browser: http://localhost:7474 (neo4j/fk2025neo4j)
#    - Qdrant API: http://localhost:6333
#    - Frontend UI: http://localhost:3000
#    - Ollama API: http://localhost:11434
#
# 3. bitcain-specific optimizations:
#    - RTX 2080 Ti GPU acceleration across all services
#    - Upgraded to llama3:8b for better AI responses
#    - Kept mxbai-embed-large for compatibility (1024 dimensions)
#    - Enhanced Ollama with flash attention and optimized memory usage
#    - PostgreSQL backend for data persistence
#    - DIRECT MCP->FastAPI integration (no n8n middleman)
#
# 4. Environment variables needed:
#    - OPENAI_API_KEY (optional)
#    - GOOGLE_API_KEY (optional)
#    - ANTHROPIC_API_KEY (optional)
#    - DOCKER_USERNAME=bitcainnet
#    - DOCKER_TOKEN (your Docker hub token)
#
# 5. Model Notes:
#    - Chat Model: llama3:8b (4.7GB VRAM) - Upgraded from 3b
#    - Embedding Model: mxbai-embed-large (1.5GB VRAM) - DO NOT CHANGE
#    - Total VRAM usage: ~8.5GB (safe for 11GB RTX 2080 Ti)
#
# 6. First-time setup:
#    The Ollama container will automatically download required models on first start.
#    This may take 10-15 minutes depending on internet speed.
#
# 7. MCP Direct Integration:
#    - Session, action, and conversation logging now goes directly to FastAPI
#    - No more silent n8n failures - immediate database persistence
#    - Check MCP health: http://localhost:8000/api/mcp/health
#    - Recent sessions: http://localhost:8000/api/mcp/sessions/recent
