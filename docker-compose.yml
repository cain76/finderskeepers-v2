# FindersKeepers v2 - Personal AI Agent Knowledge Hub for bitcain
# AI GOD MODE with Direct FastAPI Integration - NO N8N DEPENDENCY
# Optimized for RTX 2080 Ti with CUDA Toolkit
#
# ========================================
# AUTOMATIC BACKGROUND PROCESSING
# ========================================
# The FastAPI service includes automatic document processing that:
# - Starts 30 seconds after container startup
# - Processes unprocessed documents every 5 minutes
# - Handles 10 documents per batch with automatic retry
# - No manual intervention required - just docker-compose up -d
#
# To deploy on another machine:
# 1. Clone the repository
# 2. Set environment variables in .env (optional)
# 3. Run: docker-compose down && docker-compose up -d
# 4. Background processor will start automatically
# ========================================

services:
  # ========================================
  # WORKFLOW AUTOMATION - n8n (DEPRECATED - NOT USED)
  # n8n is NO LONGER USED for FindersKeepers v2
  # MCP server uses direct FastAPI integration for 100% reliability
  # Keep this running ONLY if you need it for OTHER unrelated workflows
  # ========================================
  n8n:
    image: docker.n8n.io/n8nio/n8n:latest  # Official n8n registry with latest stable version
    container_name: fk2_n8n
    restart: unless-stopped
    ports:
      - "5678:5678"
    environment:
      # Basic Authentication
      - N8N_BASIC_AUTH_ACTIVE=true
      - N8N_BASIC_AUTH_USER=${N8N_BASIC_AUTH_USER:-admin}
      - N8N_BASIC_AUTH_PASSWORD=${N8N_BASIC_AUTH_PASSWORD:-changeme}

      # Network Configuration
      - WEBHOOK_URL=http://localhost:5678/
      - N8N_PROTOCOL=http
      - N8N_HOST=localhost
      - N8N_PORT=5678

      # Security & Performance
      - GENERIC_TIMEZONE=${TZ:-America/Chicago}
      - N8N_JWT_SECRET=${N8N_JWT_SECRET:-please_change_this_jwt_secret_in_production}

      # Fix deprecation warnings
      - N8N_RUNNERS_ENABLED=true  # Enable task runners (required for future versions)
      - N8N_ENFORCE_SETTINGS_FILE_PERMISSIONS=true  # Enforce secure file permissions

      # Database Configuration (PostgreSQL)
      - DB_TYPE=postgresdb
      - DB_POSTGRESDB_HOST=postgres
      - DB_POSTGRESDB_PORT=5432
      - DB_POSTGRESDB_DATABASE=finderskeepers_v2
      - DB_POSTGRESDB_USER=finderskeepers
      - DB_POSTGRESDB_PASSWORD=${POSTGRES_PASSWORD:-changeme_postgres_password}

      # Enhanced Features
      - N8N_TEMPLATES_ENABLED=true
      - N8N_DIAGNOSTICS_ENABLED=false  # Disable telemetry for privacy
      - N8N_VERSION_NOTIFICATIONS_ENABLED=true
      - N8N_COMMUNITY_PACKAGES_ENABLED=true

      # GPU environment variables for bitcain's RTX 2080 Ti
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0

      # Workflow execution settings - REGULAR MODE FOR SINGLE INSTANCE
      - EXECUTIONS_MODE=regular
      # Queue settings disabled for regular mode
      # - QUEUE_BULL_REDIS_HOST=redis
      # - QUEUE_BULL_REDIS_PORT=6379
      # - OFFLOAD_MANUAL_EXECUTIONS_TO_WORKERS=true
      - N8N_RUNNERS_MODE=internal

      # Logging
      - N8N_LOG_LEVEL=debug
      - N8N_LOG_OUTPUT=console
      
    volumes:
      - n8n_data:/home/node/.n8n
      # Custom node modules if needed
      - ./n8n/custom-nodes:/home/node/.n8n/custom
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
    networks:
      shared-network:
        aliases:
          - n8n

  # ========================================
  # API BACKEND - AI GOD MODE with Direct Processing Pipeline
  # HANDLES ALL MCP INTEGRATION DIRECTLY - NO N8N REQUIRED
  # Automatic conversation processing with embeddings & knowledge graph
  # ========================================
  fastapi:
    build:
      context: ./services/diary-api
      dockerfile: Dockerfile
      args:
        DOCKER_USERNAME: ${DOCKER_USERNAME:-bitcainnet}
        DOCKER_TOKEN: ${DOCKER_TOKEN:-}
      x-bake:
        platforms:
          - linux/amd64
          - linux/arm64
    container_name: fk2_fastapi
    restart: unless-stopped
    ports:
      - "8000:80"
    environment:
      - BUILDX_BUILDER=${BUILDX_BUILDER:-}
      - PYTHONPATH=/app
      - LOG_LEVEL=info
      
      # TIMEZONE CONFIGURATION FOR CHICAGO CDT (bitcain)
      - TZ=America/Chicago
      - TIMEZONE=America/Chicago
      
      # Database connections for secure deployment
      - POSTGRES_URL=postgresql://finderskeepers:${POSTGRES_PASSWORD:-changeme_postgres_password}@postgres:5432/finderskeepers_v2
      - NEO4J_URI=bolt://neo4j:7687
      - NEO4J_USER=neo4j
      - NEO4J_PASSWORD=${NEO4J_PASSWORD:-changeme_neo4j_password}
      - REDIS_URL=redis://redis:6379
      - QDRANT_URL=http://qdrant:6333
      
      # Security
      - SESSION_SECRET=${SESSION_SECRET:-change_this_session_secret_in_production}
      
      # Local LLM Configuration optimized for RTX 2080 Ti
      - OLLAMA_URL=http://ollama:11434
      - USE_LOCAL_LLM=true
      - EMBEDDING_MODEL=mxbai-embed-large  # 1024 dimensions for vector search
      - CHAT_MODEL=llama3:8b  # 8B model for entity extraction
      
      # Automatic Processing Pipeline Settings
      - ENABLE_AUTO_PROCESSING=true
      - AUTO_PROCESS_CONVERSATIONS=true
      - PROCESS_BATCH_SIZE=10
      - EMBEDDING_DIMENSIONS=1024
      
      # Background Document Processor Settings for automatic processing
      # These will enable automatic document processing when deploying on any machine
      - FK2_ENABLE_BACKGROUND_PROCESSING=true       # Enable automatic processing on startup
      - FK2_PROCESSING_INTERVAL_MINUTES=5           # Process every 5 minutes
      - FK2_PROCESSING_BATCH_SIZE=10                # Process 10 documents per batch
      - FK2_PROCESSING_MAX_RETRIES=3                # Retry failed documents up to 3 times
      - FK2_PROCESSING_START_DELAY_SECONDS=30       # Wait 30 seconds after startup before processing
      
      # AI API Keys (fallback only - not used with local LLM)
      - OPENAI_API_KEY=${OPENAI_API_KEY:-}
      - GOOGLE_API_KEY=${GOOGLE_API_KEY:-}
      - ANTHROPIC_API_KEY=${ANTHROPIC_API_KEY:-}
      
      # DEPRECATED - n8n integration removed
      # - N8N_WEBHOOK_URL=http://n8n:5678
      # - N8N_API_URL=http://n8n:5678/api/v1
      
    volumes:
      - ./services/diary-api:/app
      - ./sql:/app/sql:ro
      - ./scripts:/app/scripts:ro
      - /var/run/docker.sock:/var/run/docker.sock:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    # Healthcheck temporarily disabled - service is working but endpoint needs investigation
    # healthcheck:
    #   test: ["CMD", "curl", "-f", "http://localhost:80/health" ]
    #   interval: 30s
    #   timeout: 10s
    #   retries: 5
    #   start_period: 60s
    depends_on:
      - postgres
      - neo4j
      - redis
      - qdrant
      - ollama
      # n8n removed - no longer a dependency
    networks:
      shared-network:
        aliases:
          - fastapi

  # ========================================
  # VECTOR DATABASES - Enhanced for bitcain
  # ========================================
  postgres:
    image: pgvector/pgvector:pg16
    container_name: fk2_postgres
    restart: unless-stopped
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_DB=${POSTGRES_DB:-finderskeepers_v2}
      - POSTGRES_USER=${POSTGRES_USER:-finderskeepers}
      - POSTGRES_PASSWORD=${POSTGRES_PASSWORD:-changeme_postgres_password}
      - PGDATA=/var/lib/postgresql/data/pgdata
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ['CMD-SHELL', 'pg_isready -h localhost -U ${POSTGRES_USER:-finderskeepers} -d ${POSTGRES_DB:-finderskeepers_v2}']
      interval: 5s
      timeout: 5s
      retries: 10
    command: >
      postgres -c shared_preload_libraries=vector 
               -c log_destination=stderr 
               -c log_statement=all 
               -c log_min_duration_statement=0
               -c max_connections=200
               -c shared_buffers=256MB
               -c effective_cache_size=1GB
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - postgres

  qdrant:
    image: qdrant/qdrant:latest
    container_name: fk2_qdrant
    restart: unless-stopped
    ports:
      - "6333:6333"  # HTTP API
      - "6334:6334"  # gRPC API
    environment:
      - QDRANT__SERVICE__HTTP_PORT=6333
      - QDRANT__SERVICE__GRPC_PORT=6334
      - QDRANT__LOG_LEVEL=INFO
      # GPU acceleration for vector operations
      - QDRANT__SERVICE__MAX_REQUEST_SIZE_MB=32
      - QDRANT__SERVICE__MAX_WORKERS=8
      - QDRANT__STORAGE__PERFORMANCE__MAX_SEARCH_THREADS=8
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - qdrant_data:/qdrant/storage
    deploy:
      resources:
        limits:
          memory: 2G
          cpus: '4'
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - qdrant

  # ========================================
  # KNOWLEDGE GRAPH - Enhanced for bitcain
  # ========================================
  neo4j:
    image: neo4j:5.18-community
    container_name: fk2_neo4j
    restart: unless-stopped
    ports:
      - "7474:7474"  # HTTP
      - "7687:7687"  # Bolt
    environment:
      - NEO4J_AUTH=neo4j/${NEO4J_PASSWORD:-changeme_neo4j_password}
      - NEO4J_PLUGINS=["apoc", "graph-data-science"]
      - NEO4J_dbms_security_procedures_unrestricted=apoc.*,gds.*
      - NEO4J_dbms_security_procedures_allowlist=apoc.*,gds.*
      - NEO4J_dbms_memory_heap_initial__size=512m
      - NEO4J_dbms_memory_heap_max__size=2G
      - NEO4J_dbms_memory_pagecache_size=512m
      - CUDA_VISIBLE_DEVICES=0
      - NVIDIA_VISIBLE_DEVICES=0
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - neo4j

  # ========================================
  # CACHING & SESSION MANAGEMENT
  # ========================================
  redis:
    image: redis:7-alpine
    container_name: fk2_redis
    restart: unless-stopped
    ports:
      - "6379:6379"
    command: redis-server /etc/redis/redis.conf
    volumes:
      - redis_data:/data
      - ./config/redis/redis.conf:/etc/redis/redis.conf:ro
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 5s
      timeout: 3s
      retries: 5
    networks:
      shared-network:
        aliases:
          - redis

  # ========================================
  # LOCAL LLM INFERENCE - ENHANCED CONFIGURATION
  # Optimized for RTX 2080 Ti (11GB VRAM) with July 2025 recommendations
  # ========================================
  ollama:
    image: ollama/ollama:latest
    container_name: fk2_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      # GPU Configuration
      - OLLAMA_GPU_LAYERS=999
      - CUDA_VISIBLE_DEVICES=0
      - OLLAMA_HOST=0.0.0.0
      - OLLAMA_ORIGINS=*
      
      # Optimized for RTX 2080 Ti (11GB VRAM)
      - OLLAMA_MAX_LOADED_MODELS=2
      - OLLAMA_NUM_PARALLEL=4
      - OLLAMA_MAX_QUEUE=512
      
      # PERFORMANCE OPTIMIZATIONS (July 2025)
      - OLLAMA_FLASH_ATTENTION=1         # Enable flash attention for faster inference
      - OLLAMA_GPU_MEMORY_FRACTION=0.95  # Use 95% of available VRAM
      - OLLAMA_PARALLEL_CONTEXT=2048     # Larger context window for better understanding
      - OLLAMA_BATCH_SIZE=512            # Larger batch for better throughput
      
      # Model Configuration
      - OLLAMA_DEFAULT_CHAT_MODEL=llama3:8b       # Default chat model
      - OLLAMA_DEFAULT_EMBED_MODEL=mxbai-embed-large  # Default embedding model
      
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/ollama-init.sh:/init.sh:ro  # Model initialization script
    # Run initialization script on first start
    entrypoint: ["/bin/sh", "-c", "if [ ! -f /root/.ollama/.initialized ]; then /init.sh && touch /root/.ollama/.initialized; fi && ollama serve"]
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    networks:
      shared-network:
        aliases:
          - ollama

  # ========================================
  # REACT FRONTEND - Enhanced with Admin/Maintenance Dashboard (Direct FastAPI Integration)
  # ========================================
  frontend:
    build:
      context: ./frontend
      dockerfile: Dockerfile
    container_name: fk2_frontend
    restart: unless-stopped
    ports:
      - "3000:3000"
    environment:
      - NODE_ENV=development
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000
      - VITE_NEO4J_URI=bolt://localhost:7687
      - VITE_NEO4J_USER=neo4j
      - VITE_NEO4J_PASSWORD=${NEO4J_PASSWORD:-changeme_neo4j_password}
      - VITE_QDRANT_URL=http://localhost:6333
      - CHOKIDAR_USEPOLLING=true
      - WATCHPACK_POLLING=true
      # Admin/Maintenance Dashboard Features
      - VITE_ADMIN_ENABLED=true
      - VITE_BULK_PROCESSING_ENABLED=true
      - VITE_QUEUE_MAINTENANCE_ENABLED=true
      - VITE_PROCESSING_STATS_ENABLED=true
    volumes:
      - ./frontend:/app
      - /app/node_modules
      - /var/run/docker.sock:/var/run/docker.sock:ro
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu]
    depends_on:
      - fastapi
      - neo4j
    networks:
      shared-network:
        aliases:
          - frontend

  # ========================================
  # WEB SCRAPING SERVICE
  # ========================================
  webscraper:
    build:
      context: ./services/crawl4ai-service
      dockerfile: Dockerfile
    container_name: fk2_webscraper
    restart: unless-stopped
    ports:
      - "8001:8001"
    environment:
      - PYTHONPATH=/app
    depends_on:
      - fastapi
    networks:
      shared-network:
        aliases:
          - webscraper

networks:
  shared-network:
    name: shared-network
    external: true

volumes:
  # Named volumes for data persistence
  postgres_data:
  neo4j_data:
  neo4j_logs:
  qdrant_data:
  redis_data:
  n8n_data:
  ollama_data:

# ========================================
# bitcain USAGE INSTRUCTIONS - UPDATED FOR ADMIN/MAINTENANCE INTERFACE
# ========================================
# 1. Start all services:
#    docker compose up -d
#
# 2. Access points:
#    - FastAPI Docs: http://localhost:8000/docs (NOW WITH ADMIN MCP ENDPOINTS)
#    - Admin Dashboard: http://localhost:3000/monitoring (BULK PROCESSING & QUEUE MANAGEMENT)
#    - MCP Health: http://localhost:8000/api/mcp/health
#    - Document Processing: http://localhost:8000/api/admin/processing-stats
#    - Bulk Embedding: http://localhost:8000/api/admin/bulk-embedding
#    - Queue Maintenance: http://localhost:8000/api/admin/queue-maintenance
#    - n8n Workflows: http://localhost:5678 (OPTIONAL - jeremy.cn.davis@gmail.com/Toolman7424!)
#    - Neo4j Browser: http://localhost:7474 (neo4j/fk2025neo4j)
#    - Qdrant API: http://localhost:6333
#    - Frontend UI: http://localhost:3000
#    - Ollama API: http://localhost:11434
#
# 3. ADMIN/MAINTENANCE FEATURES:
#    - Fix 12,274 unprocessed documents with bulk embedding processor
#    - Monitor processing queue status and perform maintenance
#    - Retry failed processing jobs automatically
#    - Clear completed and failed queue items
#    - Real-time processing statistics and progress tracking
#    - Project-based document processing controls
#
# 4. bitcain-specific optimizations:
#    - RTX 2080 Ti GPU acceleration across all services
#    - Upgraded to llama3:8b for better AI responses
#    - Kept mxbai-embed-large for compatibility (1024 dimensions)
#    - Enhanced Ollama with flash attention and optimized memory usage
#    - PostgreSQL backend for data persistence
#    - DIRECT MCP->FastAPI integration (no n8n middleman)
#    - ADMIN DASHBOARD for bulk operations and maintenance
#
# 5. Environment variables needed:
#    - OPENAI_API_KEY (optional)
#    - GOOGLE_API_KEY (optional)
#    - ANTHROPIC_API_KEY (optional)
#    - DOCKER_USERNAME=bitcainnet
#    - DOCKER_TOKEN (your Docker hub token)
#
# 6. Model Notes:
#    - Chat Model: llama3:8b (4.7GB VRAM) - Upgraded from 3b
#    - Embedding Model: mxbai-embed-large (1.5GB VRAM) - DO NOT CHANGE
#    - Total VRAM usage: ~8.5GB (safe for 11GB RTX 2080 Ti)
#
# 7. First-time setup:
#    The Ollama container will automatically download required models on first start.
#    This may take 10-15 minutes depending on internet speed.
#
# 8. MCP Direct Integration:
#    - Session, action, and conversation logging now goes directly to FastAPI
#    - No more silent n8n failures - immediate database persistence
#    - Check MCP health: http://localhost:8000/api/mcp/health
#    - Recent sessions: http://localhost:8000/api/mcp/sessions/recent
#
# 9. ADMIN PANEL USAGE:
#    - Navigate to http://localhost:3000/monitoring
#    - Click "Admin Panel" button to access maintenance controls
#    - Use "Start Bulk Embedding" to process unembedded documents
#    - Use queue maintenance to clean up processing backlogs
#
# 10. AUTOMATIC BACKGROUND PROCESSING (NEW!):
#    The system now includes automatic document processing that runs continuously.
#    
#    DEPLOYMENT ON ANOTHER MACHINE:
#    - Clone the repository: git clone <repo-url>
#    - Navigate to project: cd finderskeepers-v2
#    - Start services: docker-compose up -d
#    - The background processor will automatically:
#      * Wait 30 seconds for all services to initialize
#      * Process 10 unprocessed documents every 5 minutes
#      * Retry failed documents up to 3 times
#      * Log all processing to docker logs fk2_fastapi
#    
#    MONITORING:
#    - Check processing status: docker logs fk2_fastapi --tail 50 | grep -E "Processing|Batch"
#    - View processing stats: curl http://localhost:8000/api/v1/stats
#    - Disable if needed: Set FK2_ENABLE_BACKGROUND_PROCESSING=false in docker-compose.yml
#
#    NO MANUAL INTERVENTION REQUIRED - Just docker-compose up -d and it works!
#    - Monitor real-time processing statistics and completion rates
