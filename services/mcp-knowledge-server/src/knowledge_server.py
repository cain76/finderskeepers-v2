#!/usr/bin/env python3
"""
FindersKeepers v2 MCP Knowledge Server
Provides agents with natural language access to the knowledge base

This server exposes tools, resources, and prompts that allow AI agents to:
- Search documents using semantic similarity
- Query the knowledge graph for relationships
- Retrieve agent session context and history
- Analyze document similarity and connections
"""

import asyncio
import logging
import os
import uuid
from typing import Any, Dict, List, Optional
from datetime import datetime
import json
import psutil
from dotenv import load_dotenv

from mcp.server.fastmcp import FastMCP
from mcp.types import Resource, Tool, Prompt

from database.postgres_client import PostgresClient
from database.neo4j_client import Neo4jClient  
from database.qdrant_client import QdrantClient
from database.redis_client import RedisClient
from activity_logger import activity_logger
from conversation_monitor import setup_conversation_monitoring
from conversation_logging_middleware import initialize_conversation_middleware, get_conversation_middleware

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Initialize MCP server
mcp = FastMCP("FindersKeepers Knowledge Server")

# Database clients
postgres = PostgresClient()
neo4j = Neo4jClient()
qdrant = QdrantClient()
redis = RedisClient()

# Global conversation monitor for exit command detection
conversation_monitor = None

# Parent process monitoring
parent_process_monitor_task = None

# ========================================
# TOOLS - Agent-callable functions
# ========================================

# Universal middleware wrapper for all MCP tools
async def _log_tool_call(tool_name: str, params: Dict[str, Any], result: Any = None):
    """Universal middleware logging for all MCP tool calls"""
    try:
        middleware = get_conversation_middleware()
        if middleware and activity_logger.session_id:
            # Log the request (user intent)
            await middleware.on_message(
                message={
                    "method": tool_name,
                    "params": params
                },
                session_id=activity_logger.session_id
            )
            
            # If we have a result, log the response
            if result is not None:
                await middleware.on_message(
                    message={
                        "result": result,
                        "type": "response"
                    },
                    session_id=activity_logger.session_id
                )
                
    except Exception as e:
        logger.error(f"Error in universal tool logging: {e}")

@mcp.tool()
async def search_documents(
    query: str,
    project: Optional[str] = None,
    tags: Optional[List[str]] = None,
    limit: int = 10,
    min_score: float = 0.5
) -> Dict[str, Any]:
    """
    Search documents using semantic search across all knowledge sources.
    
    Args:
        query: Natural language search query
        project: Filter by project name (optional)
        tags: Filter by document tags (optional)
        limit: Maximum number of results (default: 10)
        min_score: Minimum similarity score 0.0-1.0 (default: 0.5)
    
    Returns:
        Dictionary with search results, metadata, and search statistics
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("search_documents", {
        "query": query, "project": project, "tags": tags, 
        "limit": limit, "min_score": min_score
    })
    
    try:
        logger.info(f"üîç Searching documents: '{query}' (project: {project}, tags: {tags})")
        
        # Generate embeddings for the query using FastAPI backend
        embeddings = await qdrant.generate_embeddings(query)
        
        if not embeddings:
            return {
                "error": "Failed to generate embeddings for query",
                "query": query,
                "results": []
            }
        
        # Search in Qdrant vector database
        vector_results = await qdrant.search(
            embeddings=embeddings,
            project=project,
            tags=tags,
            limit=limit,
            min_score=min_score
        )
        
        # Enhance results with PostgreSQL metadata
        enhanced_results = []
        for result in vector_results:
            doc_metadata = await postgres.get_document_metadata(result["document_id"])
            enhanced_results.append({
                "content": result["content"],
                "score": result["score"],
                "document_id": result["document_id"],
                "chunk_id": result["chunk_id"],
                "metadata": doc_metadata,
                "preview": result["content"][:200] + "..." if len(result["content"]) > 200 else result["content"]
            })
        
        result = {
            "query": query,
            "total_results": len(enhanced_results),
            "results": enhanced_results,
            "search_params": {
                "project": project, 
                "tags": tags,
                "limit": limit,
                "min_score": min_score
            },
            "embeddings_used": len(embeddings) > 0,
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Log successful search to n8n
        await activity_logger.log_tool_call(
            tool_name="search_documents",
            arguments={"query": query, "project": project, "tags": tags, "limit": limit},
            result=result
        )
        
        # Log the successful result through universal middleware
        await _log_tool_call("search_documents", {}, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Document search failed: {e}")
        await activity_logger.log_error("search_documents", e)
        return {
            "error": str(e),
            "query": query,
            "results": [],
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def query_knowledge_graph(
    question: str,
    entity_types: Optional[List[str]] = None,
    max_depth: int = 2,
    limit: int = 20
) -> Dict[str, Any]:
    """
    Query the Neo4j knowledge graph for entity relationships and context.
    
    Args:
        question: Natural language question about relationships
        entity_types: Types of entities to focus on (optional)
        max_depth: Maximum traversal depth in graph (default: 2)
        limit: Maximum number of results (default: 20)
    
    Returns:
        Graph query results with entities, relationships, and insights
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("query_knowledge_graph", {
        "question": question, "entity_types": entity_types,
        "max_depth": max_depth, "limit": limit
    })
    
    try:
        logger.info(f"üß† Querying knowledge graph: '{question}'")
        
        # Extract keywords from the question for entity matching
        keywords = await neo4j.extract_keywords(question)
        
        # Build and execute Cypher query
        graph_results = await neo4j.query_entities_and_relationships(
            keywords=keywords,
            entity_types=entity_types,
            max_depth=max_depth,
            limit=limit
        )
        
        result = {
            "question": question,
            "keywords_extracted": keywords,
            "graph_results": graph_results,
            "query_params": {
                "entity_types": entity_types,
                "max_depth": max_depth,
                "limit": limit
            },
            "entities_found": len(graph_results.get("entities", [])),
            "relationships_found": len(graph_results.get("relationships", [])),
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Log knowledge graph query
        await activity_logger.log_tool_call(
            tool_name="query_knowledge_graph",
            arguments={"question": question, "entity_types": entity_types},
            result=result
        )
        
        # Log the successful result through universal middleware
        await _log_tool_call("query_knowledge_graph", {}, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Knowledge graph query failed: {e}")
        return {
            "error": str(e),
            "question": question,
            "graph_results": {},
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def get_full_conversation_context(
    session_id: Optional[str] = None,
    agent_type: Optional[str] = None,
    project: Optional[str] = None,
    recent_actions: int = 10,
    include_files: bool = True,
    include_conversation_history: bool = True,
    conversation_limit: int = 50
) -> Dict[str, Any]:
    """
    Retrieve complete session context including conversation history from the knowledge base.
    
    Args:
        session_id: Specific session to retrieve (optional)
        agent_type: Filter by agent type like 'claude', 'gpt' (optional)
        project: Filter by project context (optional)
        recent_actions: Number of recent actions to include (default: 10)
        include_files: Whether to include file change information (default: True)
        include_conversation_history: Whether to include full conversation messages (default: True)
        conversation_limit: Maximum conversation messages to retrieve (default: 50)
    
    Returns:
        Complete session context with actions, conversation history, metadata, and file changes
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("get_full_conversation_context", {
        "session_id": session_id, "agent_type": agent_type, "project": project,
        "recent_actions": recent_actions, "include_files": include_files, 
        "include_conversation_history": include_conversation_history, "conversation_limit": conversation_limit
    })
    
    try:
        logger.info(f"üìã Getting session context: {session_id or 'recent sessions'}")
        
        if session_id:
            # Get specific session
            session = await postgres.get_session(session_id)
            actions = await postgres.get_session_actions(session_id, limit=recent_actions)
        else:
            # Get recent sessions based on filters
            sessions = await postgres.get_recent_sessions(
                agent_type=agent_type,
                project=project,
                limit=3
            )
            session = sessions[0] if sessions else None
            actions = await postgres.get_recent_actions(
                agent_type=agent_type,
                project=project,
                limit=recent_actions
            )
        
        # Get file changes if requested
        file_changes = []
        if include_files and actions:
            file_changes = await postgres.get_files_affected_by_actions(
                [action["action_id"] for action in actions]
            )
        
        # Get conversation history if requested
        conversation_messages = []
        conversation_summary = {}
        if include_conversation_history and session_id:
            # Get conversation messages directly from database
            conversation_messages = await postgres.get_conversation_messages(
                session_id=session_id,
                limit=conversation_limit
            )
            
            # Generate conversation summary
            user_messages = [m for m in conversation_messages if m["message_type"] == "user_message"]
            ai_responses = [m for m in conversation_messages if m["message_type"] == "ai_response"]
            
            conversation_summary = {
                "total_messages": len(conversation_messages),
                "user_messages": len(user_messages),
                "ai_responses": len(ai_responses),
                "dominant_emotions": get_dominant_emotions(conversation_messages),
                "key_topics": get_key_topics(conversation_messages),
                "conversation_flow": analyze_conversation_flow(conversation_messages),
                "last_user_message": user_messages[0]["content"][:200] if user_messages else None,
                "last_ai_response": ai_responses[0]["content"][:200] if ai_responses else None
            }
        
        result = {
            "session": session,
            "actions": actions,
            "file_changes": file_changes if include_files else [],
            "conversation_history": conversation_messages if include_conversation_history else [],
            "conversation_summary": conversation_summary if include_conversation_history else {},
            "summary": {
                "session_id": session["session_id"] if session else None,
                "agent_type": session["agent_type"] if session else None,
                "project": session["project"] if session else None,
                "action_count": len(actions),
                "files_affected": len(file_changes) if include_files else 0,
                "success_rate": sum(1 for a in actions if a.get("success", True)) / len(actions) if actions else 0
            },
            "filters": {
                "session_id": session_id,
                "agent_type": agent_type,
                "project": project
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Log the successful result through universal middleware
        await _log_tool_call("get_full_conversation_context", {}, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Session context retrieval failed: {e}")
        return {
            "error": str(e),
            "session": None,
            "actions": [],
            "file_changes": [],
            "conversation_history": [],
            "conversation_summary": {},
            "summary": {
                "session_id": None,
                "agent_type": None,
                "project": None,
                "action_count": 0,
                "files_affected": 0,
                "success_rate": 0
            },
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def analyze_document_similarity(
    document_id: str,
    similarity_threshold: float = 0.7,
    limit: int = 8,
    include_content: bool = False
) -> Dict[str, Any]:
    """
    Find documents similar to a given document using vector similarity.
    
    Args:
        document_id: ID of the reference document
        similarity_threshold: Minimum similarity score 0.0-1.0 (default: 0.7)
        limit: Maximum number of similar documents (default: 8)
        include_content: Whether to include full content in results (default: False)
    
    Returns:
        List of similar documents with similarity scores and metadata
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("analyze_document_similarity", {
        "document_id": document_id, "similarity_threshold": similarity_threshold,
        "limit": limit, "include_content": include_content
    })
    
    try:
        logger.info(f"üîó Analyzing similarity for document: {document_id}")
        
        # Get document embeddings from Qdrant
        doc_embeddings = await qdrant.get_document_embeddings(document_id)
        
        if not doc_embeddings:
            return {
                "error": "Document not found or no embeddings available",
                "document_id": document_id,
                "similar_documents": []
            }
        
        # Find similar documents
        similar_docs = await qdrant.find_similar_documents(
            embeddings=doc_embeddings,
            threshold=similarity_threshold,
            limit=limit,
            exclude_document_id=document_id
        )
        
        # Enhance with metadata from PostgreSQL
        enhanced_results = []
        for doc in similar_docs:
            metadata = await postgres.get_document_metadata(doc["document_id"])
            result = {
                "document_id": doc["document_id"],
                "similarity_score": doc["score"],
                "metadata": metadata,
                "preview": doc["content"][:150] + "..." if not include_content and len(doc["content"]) > 150 else None
            }
            
            if include_content:
                result["content"] = doc["content"]
                
            enhanced_results.append(result)
        
        # Get reference document metadata
        reference_doc = await postgres.get_document_metadata(document_id)
        
        result = {
            "reference_document": {
                "document_id": document_id,
                "metadata": reference_doc
            },
            "similarity_threshold": similarity_threshold,
            "similar_documents": enhanced_results,
            "analysis": {
                "total_found": len(enhanced_results),
                "avg_similarity": sum(d["similarity_score"] for d in enhanced_results) / len(enhanced_results) if enhanced_results else 0,
                "highest_similarity": max((d["similarity_score"] for d in enhanced_results), default=0)
            },
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Log the successful result through universal middleware
        await _log_tool_call("analyze_document_similarity", {}, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Document similarity analysis failed: {e}")
        return {
            "error": str(e),
            "document_id": document_id,
            "similar_documents": [],
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def initialize_claude_session(
    user_id: str = "local_user",
    project: str = "finderskeepers-v2",
    context: Optional[Dict[str, Any]] = None
) -> Dict[str, Any]:
    """
    Initialize a new Claude Code session with automatic logging and context retrieval.
    
    Args:
        user_id: User identifier (default: local_user)
        project: Project context (default: finderskeepers-v2)
        context: Additional session context (optional)
    
    Returns:
        Session information with current project status and recent activity summary
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("initialize_claude_session", {
        "user_id": user_id, "project": project, "context": context
    })
    
    try:
        logger.info(f"üöÄ Initializing Claude Code session for project: {project}")
        
        # CRITICAL FIX: Check for active sessions first before creating a new one
        active_sessions = await postgres.get_active_sessions(project=project, limit=1)
        
        if active_sessions:
            active_session = active_sessions[0]
            logger.info(f"üîÑ Found active session: {active_session['session_id']}, returning existing session")
            
            # Return information about the active session instead of creating a new one
            return {
                "session_id": active_session['session_id'],
                "status": "existing_session",
                "project": project,
                "current_context": await get_full_conversation_context(
                    session_id=active_session['session_id'],
                    include_files=True,
                    include_conversation_history=True
                ),
                "message": f"‚úÖ Using existing active session {active_session['session_id'][-8:]}",
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Create session data for Claude Code
        session_context = {
            "client": "claude_code",
            "version": "latest",
            "started_at": datetime.utcnow().isoformat(),
            "connection_type": "mcp_tool_call",
            "auto_logging_enabled": True,
            "purpose": "Interactive development session",
            **(context or {})
        }
        
        session_data = {
            "agent_type": "claude-code",
            "user_id": user_id,
            "project": project,
            "context": session_context
        }
        
        # Call n8n Agent Session Logger webhook to create session
        import httpx
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{activity_logger.n8n_webhook_url}/webhook/agent-logger",
                json=session_data,
                timeout=10.0
            )
            
            session_id = None
            if response.status_code == 200:
                result = response.json()
                if isinstance(result, list) and len(result) > 0:
                    session_response = result[0]
                    session_id = session_response.get("session_id")
            
            if not session_id:
                session_id = f"claude_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
                
        # Set the session ID in the activity logger for tracking
        activity_logger.set_session_id(session_id)
                
        # Get current project context and recent activity with FULL conversation history
        context_data = await get_full_conversation_context(
            project=project,
            recent_actions=20,
            include_files=True,
            include_conversation_history=True,
            conversation_limit=30
        )
        
        # Get project overview
        project_overview = await get_project_overview(
            project=project,
            include_recent_activity=True,
            activity_days=7
        )
        
        # Log the session initialization
        await activity_logger.log_action(
            action_type="claude_session_init",
            description=f"Claude Code session initialized for project {project}",
            details={
                "session_id": session_id,
                "project": project,
                "user_id": user_id,
                "initialization_method": "mcp_tool",
                "context_retrieved": True,
                "recent_sessions_found": len(context_data.get("actions", [])),
                "project_activity_level": project_overview.get("overview", {}).get("activity_level", "unknown")
            },
            success=True
        )
        
        return {
            "session_id": session_id,
            "status": "initialized",
            "project": project,
            "current_context": context_data,
            "project_overview": project_overview,
            "recommendations": generate_continuation_recommendations(context_data, project_overview),
            "message": f"‚úÖ Claude Code session initialized! You have context from {len(context_data.get('actions', []))} recent actions.",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Claude session initialization failed: {e}")
        await activity_logger.log_error("claude_session_init", e)
        return {
            "error": str(e),
            "status": "failed",
            "project": project,
            "timestamp": datetime.utcnow().isoformat()
        }

def generate_continuation_recommendations(context_data: Dict[str, Any], project_overview: Dict[str, Any]) -> List[str]:
    """Generate intelligent recommendations for continuing work based on recent activity"""
    recommendations = []
    
    # Analyze recent actions
    recent_actions = context_data.get("actions", [])
    if recent_actions:
        latest_action = recent_actions[0]
        action_type = latest_action.get("action_type", "")
        
        if "file_edit" in action_type or "fix" in action_type.lower():
            recommendations.append("üîß Continue fixing issues or implementing changes from the last session")
        
        if "commit" in action_type.lower():
            recommendations.append("üöÄ Consider pushing changes or starting new features")
            
        if "error" in action_type.lower() or not latest_action.get("success", True):
            recommendations.append("‚ö†Ô∏è Address the error from the previous session")
    
    # Analyze file changes
    file_changes = context_data.get("file_changes", [])
    if file_changes:
        recommendations.append(f"üìÑ Review changes to {len(file_changes)} files from recent sessions")
    
    # Analyze project activity level
    activity_level = project_overview.get("overview", {}).get("activity_level", "")
    if activity_level == "high":
        recommendations.append("üî• High activity detected - review recent progress before continuing")
    elif activity_level == "low":
        recommendations.append("üå± Low recent activity - good time to start new features or major changes")
    
    # Default recommendation
    if not recommendations:
        recommendations.append("üéØ Check project status and plan next development steps")
        
    return recommendations[:3]  # Limit to top 3 recommendations

@mcp.tool()
async def log_conversation_message(
    session_id: str,
    message_type: str,  # "user_message" or "ai_response"
    content: str,
    context: Optional[Dict[str, Any]] = None,
    reasoning: Optional[str] = None,
    tools_used: Optional[List[str]] = None,
    files_referenced: Optional[List[str]] = None
) -> Dict[str, Any]:
    """
    Log detailed conversation messages for complete context preservation.
    
    Args:
        session_id: Session ID from initialize_claude_session
        message_type: "user_message", "ai_response", "system_message", "tool_result"
        content: Full message content
        context: Additional context (emotional tone, urgency, topic changes)
        reasoning: AI reasoning process and decision-making (for AI responses)
        tools_used: List of tools used in this interaction
        files_referenced: Files mentioned or referenced in the message
    
    Returns:
        Message logging confirmation with message_id
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("log_conversation_message", {
        "session_id": session_id, "message_type": message_type, "content": content[:100],
        "context": context, "reasoning": reasoning, "tools_used": tools_used, "files_referenced": files_referenced
    })
    
    try:
        logger.info(f"üí¨ Logging conversation message: {message_type} for session {session_id}")
        
        message_data = {
            "session_id": session_id,
            "action_type": f"conversation:{message_type}",
            "description": f"Conversation {message_type}: {content[:100]}..." if len(content) > 100 else content,
            "details": {
                "message_type": message_type,
                "full_content": content,
                "content_length": len(content),
                "context": context or {},
                "reasoning": reasoning,
                "tools_used": tools_used or [],
                "files_referenced": files_referenced or [],
                "emotional_indicators": extract_emotional_context(content),
                "topic_keywords": extract_topic_keywords(content),
                "conversation_flow": "continuation" if message_type == "user_message" else "response"
            },
            "files_affected": files_referenced or [],
            "success": True
        }
        
        # Check for exit commands in user messages
        global conversation_monitor
        if conversation_monitor and message_type == "user_message":
            exit_detected = await conversation_monitor.process_message(content, message_type)
            if exit_detected:
                # Exit was detected and processed, add this to the message details
                message_data["details"]["exit_command_detected"] = True
        
        # Generate message ID first
        message_id = f"msg_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # Store directly to database first (critical for data persistence)
        try:
            await postgres.create_conversation_message(
                message_id=message_id,
                session_id=session_id,
                message_type=message_type,
                content=content,
                context=context,
                reasoning=reasoning,
                tools_used=tools_used,
                files_referenced=files_referenced
            )
            logger.info(f"‚úÖ Conversation message {message_id} stored to database")
        except Exception as db_error:
            logger.error(f"Database storage failed for message {message_id}: {db_error}")
        
        # Call n8n Agent Action Tracker webhook for conversation logging (secondary)
        import httpx
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{activity_logger.n8n_webhook_url}/webhook/agent-actions",
                    json={**message_data, "action_id": message_id},
                    timeout=10.0
                )
                
                if response.status_code == 200:
                    logger.info(f"‚úÖ Conversation message {message_id} sent to n8n webhook")
                else:
                    logger.warning(f"n8n webhook failed for message {message_id}: {response.status_code}")
        except Exception as webhook_error:
            logger.warning(f"n8n webhook error for message {message_id}: {webhook_error}")
        
        return {
            "message_id": message_id,
            "session_id": session_id,
            "status": "logged",
            "message_type": message_type,
            "content_length": len(content),
            "message": f"‚úÖ Conversation {message_type} logged successfully",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Conversation logging failed: {e}")
        return {
            "error": str(e),
            "status": "failed",
            "session_id": session_id,
            "message_type": message_type,
            "timestamp": datetime.utcnow().isoformat()
        }

def extract_emotional_context(content: str) -> List[str]:
    """Extract emotional indicators from message content"""
    emotional_indicators = []
    content_lower = content.lower()
    
    if any(word in content_lower for word in ["!!", "excited", "amazing", "awesome", "love"]):
        emotional_indicators.append("high_enthusiasm")
    if any(word in content_lower for word in ["frustrated", "annoying", "hate", "terrible"]):
        emotional_indicators.append("frustration")
    if "?" in content and any(word in content_lower for word in ["how", "what", "why", "when", "where"]):
        emotional_indicators.append("inquiry")
    if any(word in content_lower for word in ["god", "ultimate", "all-knowing", "perfect"]):
        emotional_indicators.append("aspiration")
    if "MAJOR" in content or content.isupper():
        emotional_indicators.append("emphasis")
        
    return emotional_indicators

def extract_topic_keywords(content: str) -> List[str]:
    """Extract key topics and concepts from message content"""
    import re
    
    # Technical keywords
    tech_keywords = re.findall(r'\b(?:docker|gpu|cuda|mcp|session|logging|database|ai|llm|ollama|fastapi|neo4j|qdrant)\b', content.lower())
    
    # Action keywords  
    action_keywords = re.findall(r'\b(?:implement|fix|create|add|update|configure|test|deploy|commit|push)\b', content.lower())
    
    # Concept keywords
    concept_keywords = re.findall(r'\b(?:conversation|context|memory|knowledge|intelligence|automation|integration)\b', content.lower())
    
    return list(set(tech_keywords + action_keywords + concept_keywords))

def get_dominant_emotions(conversation_messages: List[Dict[str, Any]]) -> List[str]:
    """Analyze dominant emotional patterns in conversation"""
    emotion_counts = {}
    for message in conversation_messages:
        for emotion in message.get("emotional_indicators", []):
            emotion_counts[emotion] = emotion_counts.get(emotion, 0) + 1
    
    # Return top 3 emotions
    sorted_emotions = sorted(emotion_counts.items(), key=lambda x: x[1], reverse=True)
    return [emotion for emotion, count in sorted_emotions[:3]]

def get_key_topics(conversation_messages: List[Dict[str, Any]]) -> List[str]:
    """Extract key topics discussed in conversation"""
    topic_counts = {}
    for message in conversation_messages:
        for topic in message.get("topic_keywords", []):
            topic_counts[topic] = topic_counts.get(topic, 0) + 1
    
    # Return top 5 topics
    sorted_topics = sorted(topic_counts.items(), key=lambda x: x[1], reverse=True)
    return [topic for topic, count in sorted_topics[:5]]

def analyze_conversation_flow(conversation_messages: List[Dict[str, Any]]) -> str:
    """Analyze the flow and pattern of the conversation"""
    if not conversation_messages:
        return "no_conversation"
    
    user_msg_count = len([m for m in conversation_messages if m["message_type"] == "user_message"])
    ai_msg_count = len([m for m in conversation_messages if m["message_type"] == "ai_response"])
    
    if user_msg_count > ai_msg_count:
        return "user_driven"
    elif ai_msg_count > user_msg_count:
        return "ai_driven"
    else:
        return "balanced_dialogue"

# ========================================
# ENHANCED SESSION CONTINUITY HELPERS
# ========================================

def generate_detailed_actions_summary(actions: List[Dict[str, Any]]) -> Dict[str, Any]:
    """Generate a detailed summary of recent actions for session resumption"""
    if not actions:
        return {"message": "No recent actions found", "categories": {}}
    
    # Categorize actions by type
    action_categories = {}
    for action in actions:
        action_type = action.get("action_type", "unknown")
        category = action_type.split(":")[0] if ":" in action_type else action_type
        
        if category not in action_categories:
            action_categories[category] = []
        action_categories[category].append(action)
    
    # Generate summary for each category
    category_summaries = {}
    for category, category_actions in action_categories.items():
        successful = [a for a in category_actions if a.get("success", True)]
        failed = [a for a in category_actions if not a.get("success", True)]
        
        category_summaries[category] = {
            "total": len(category_actions),
            "successful": len(successful),
            "failed": len(failed),
            "success_rate": len(successful) / len(category_actions) if category_actions else 0,
            "latest_action": category_actions[0] if category_actions else None,
            "latest_successful": successful[0] if successful else None,
            "latest_failed": failed[0] if failed else None
        }
    
    # Overall summary
    total_successful = sum(len([a for a in actions if a.get("success", True)]) for actions in [actions])
    overall_success_rate = total_successful / len(actions) if actions else 0
    
    return {
        "total_actions": len(actions),
        "overall_success_rate": overall_success_rate,
        "categories": category_summaries,
        "most_recent": actions[0] if actions else None,
        "action_types": list(action_categories.keys()),
        "time_span": {
            "latest": actions[0].get("timestamp") if actions else None,
            "earliest": actions[-1].get("timestamp") if actions else None
        }
    }

def generate_intelligent_next_steps(session_data: Dict[str, Any], actions: List[Dict[str, Any]]) -> List[str]:
    """Generate intelligent next steps based on session analysis"""
    next_steps = []
    
    if not actions:
        return ["üéØ Review project roadmap", "üìã Check recent commits", "üîç Search knowledge base"]
    
    # Analyze recent actions for patterns
    recent_actions = actions[:5]
    failed_actions = [a for a in recent_actions if not a.get("success", True)]
    
    # Check for unresolved errors
    if failed_actions:
        latest_error = failed_actions[0]
        next_steps.append(f"üîß Resolve error: {latest_error.get('description', 'Unknown error')}")
    
    # Check for file modifications
    file_actions = [a for a in recent_actions if "file" in a.get("action_type", "").lower()]
    if file_actions:
        next_steps.append("üìÑ Review recent file changes and test functionality")
    
    # Check for configuration changes
    config_actions = [a for a in recent_actions if "config" in a.get("action_type", "").lower()]
    if config_actions:
        next_steps.append("‚öôÔ∏è Verify configuration changes are working correctly")
    
    # Check for database/MCP actions
    db_actions = [a for a in recent_actions if any(keyword in a.get("action_type", "").lower() 
                                                  for keyword in ["database", "mcp", "session"])]
    if db_actions:
        next_steps.append("üóÑÔ∏è Test database connections and MCP functionality")
    
    # Check for commit/deployment actions
    commit_actions = [a for a in recent_actions if "commit" in a.get("action_type", "").lower()]
    if commit_actions:
        next_steps.append("üöÄ Consider pushing changes or deploying updates")
    
    # Default recommendations if no specific patterns found
    if not next_steps:
        next_steps.extend([
            "üéØ Continue with current development tasks",
            "üìã Check project status and priorities",
            "üîç Review recent changes and test functionality"
        ])
    
    return next_steps[:4]  # Limit to top 4 recommendations

def generate_continuation_recommendations_enhanced(session_data: Dict[str, Any], actions: List[Dict[str, Any]]) -> List[str]:
    """Generate enhanced continuation recommendations with priority and context"""
    recommendations = []
    
    if not actions:
        return ["üéØ Start with project overview", "üìã Review roadmap", "üîç Check knowledge base"]
    
    # Analyze success patterns
    successful_actions = [a for a in actions if a.get("success", True)]
    failed_actions = [a for a in actions if not a.get("success", True)]
    
    success_rate = len(successful_actions) / len(actions) if actions else 0
    
    if success_rate < 0.7:
        recommendations.append("‚ö†Ô∏è HIGH PRIORITY: Address recent errors to improve success rate")
    
    # Check for incomplete work
    incomplete_indicators = ["partial", "started", "in_progress", "todo"]
    incomplete_actions = [a for a in actions if any(indicator in a.get("description", "").lower() 
                                                   for indicator in incomplete_indicators)]
    
    if incomplete_actions:
        recommendations.append("üîÑ Complete partially finished tasks from previous session")
    
    # Check for testing needs
    test_actions = [a for a in actions if "test" in a.get("action_type", "").lower()]
    code_actions = [a for a in actions if any(keyword in a.get("action_type", "").lower() 
                                             for keyword in ["file_edit", "code", "implement"])]
    
    if code_actions and not test_actions:
        recommendations.append("üß™ Run tests to verify recent code changes")
    
    # Check for documentation needs
    doc_actions = [a for a in actions if "doc" in a.get("action_type", "").lower()]
    if not doc_actions and len(actions) > 5:
        recommendations.append("üìö Consider documenting recent changes")
    
    # Performance and optimization
    if len(actions) > 10:
        recommendations.append("‚ö° Review session performance and optimize workflow")
    
    # Default productive recommendations
    if not recommendations:
        recommendations.extend([
            "üéØ Continue with current development momentum",
            "üìã Plan next feature or improvement",
            "üîç Explore knowledge base for inspiration"
        ])
    
    return recommendations[:3]  # Limit to top 3 recommendations

def identify_priority_items(actions: List[Dict[str, Any]], files_modified: List[str]) -> List[str]:
    """Identify high-priority items that need immediate attention"""
    priority_items = []
    
    # Check for failed actions
    failed_actions = [a for a in actions if not a.get("success", True)]
    if failed_actions:
        priority_items.append(f"üö® {len(failed_actions)} failed action(s) need attention")
    
    # Check for critical files
    critical_files = [f for f in files_modified if any(keyword in f.lower() 
                                                      for keyword in ["config", "server", "main", "init"])]
    if critical_files:
        priority_items.append(f"‚ö†Ô∏è {len(critical_files)} critical file(s) modified")
    
    # Check for security-related changes
    security_actions = [a for a in actions if any(keyword in a.get("description", "").lower() 
                                                  for keyword in ["auth", "security", "permission", "token"])]
    if security_actions:
        priority_items.append("üîí Security-related changes need verification")
    
    # Check for database changes
    db_actions = [a for a in actions if any(keyword in a.get("action_type", "").lower() 
                                           for keyword in ["database", "migration", "schema"])]
    if db_actions:
        priority_items.append("üóÑÔ∏è Database changes require testing")
    
    return priority_items

def identify_session_warnings(actions: List[Dict[str, Any]], failed_actions: List[Dict[str, Any]]) -> List[str]:
    """Identify potential warnings or issues from the session"""
    warnings = []
    
    # High failure rate warning
    if failed_actions and len(failed_actions) / len(actions) > 0.3:
        warnings.append("‚ö†Ô∏è High failure rate detected - review approach")
    
    # Repeated failures warning
    if len(failed_actions) > 3:
        warnings.append("üîÑ Multiple failures detected - may need different strategy")
    
    # No recent activity warning
    if actions and actions[0].get("timestamp"):
        from datetime import datetime, timedelta
        last_action_time = datetime.fromisoformat(actions[0]["timestamp"].replace("Z", "+00:00"))
        if datetime.now(last_action_time.tzinfo) - last_action_time > timedelta(hours=6):
            warnings.append("‚è∞ Session has been inactive for over 6 hours")
    
    # Resource-intensive actions warning
    resource_actions = [a for a in actions if any(keyword in a.get("description", "").lower() 
                                                  for keyword in ["build", "compile", "install", "download"])]
    if len(resource_actions) > 5:
        warnings.append("üíæ Multiple resource-intensive actions detected")
    
    return warnings

def analyze_session_conversation_flow(actions: List[Dict[str, Any]]) -> str:
    """Analyze conversation flow patterns from actions"""
    conversation_actions = [a for a in actions if "conversation" in a.get("action_type", "")]
    
    if not conversation_actions:
        return "no_conversation_data"
    
    user_messages = [a for a in conversation_actions if "user_message" in a.get("action_type", "")]
    ai_responses = [a for a in conversation_actions if "ai_response" in a.get("action_type", "")]
    
    if len(user_messages) > len(ai_responses):
        return "user_driven"
    elif len(ai_responses) > len(user_messages):
        return "ai_driven"
    else:
        return "balanced_interaction"

def analyze_session_completion_status(actions: List[Dict[str, Any]]) -> str:
    """Analyze whether the session appears to have completed its goals"""
    if not actions:
        return "no_activity"
    
    # Check for completion indicators
    completion_indicators = ["complete", "finished", "done", "success", "resolved"]
    completion_actions = [a for a in actions if any(indicator in a.get("description", "").lower() 
                                                   for indicator in completion_indicators)]
    
    # Check for failure indicators
    failure_indicators = ["failed", "error", "broken", "issue"]
    failure_actions = [a for a in actions if any(indicator in a.get("description", "").lower() 
                                                 for indicator in failure_indicators)]
    
    success_rate = len([a for a in actions if a.get("success", True)]) / len(actions)
    
    if completion_actions and success_rate > 0.8:
        return "likely_complete"
    elif failure_actions and success_rate < 0.5:
        return "incomplete_with_issues"
    elif success_rate > 0.7:
        return "progressing_well"
    else:
        return "mixed_progress"

def calculate_session_duration(session_data: Dict[str, Any]) -> str:
    """Calculate human-readable session duration"""
    started_at = session_data.get("started_at")
    ended_at = session_data.get("ended_at")
    
    if not started_at:
        return "unknown"
    
    from datetime import datetime
    try:
        start_time = datetime.fromisoformat(started_at.replace("Z", "+00:00"))
        end_time = datetime.fromisoformat(ended_at.replace("Z", "+00:00")) if ended_at else datetime.now(start_time.tzinfo)
        
        duration = end_time - start_time
        hours = duration.total_seconds() / 3600
        
        if hours < 1:
            return f"{int(duration.total_seconds() / 60)} minutes"
        elif hours < 24:
            return f"{hours:.1f} hours"
        else:
            return f"{int(hours / 24)} days, {int(hours % 24)} hours"
    except:
        return "unknown"

async def check_pending_operations(session_id: str) -> bool:
    """Check if there are any pending operations for the session"""
    try:
        # Check for pending webhook calls
        pending_webhooks = await redis.get(f"pending_webhooks:{session_id}")
        
        # Check for pending database writes
        pending_writes = await redis.get(f"pending_writes:{session_id}")
        
        # Check for pending file operations
        pending_files = await redis.get(f"pending_files:{session_id}")
        
        return any([pending_webhooks, pending_writes, pending_files])
    except:
        return False

async def cleanup_zombie_sessions(max_age_hours: int = 6, dry_run: bool = False) -> Dict[str, Any]:
    """
    Clean up zombie sessions that have been active for too long
    
    Args:
        max_age_hours: Maximum age in hours before a session is considered zombie
        dry_run: If True, only identify zombie sessions without ending them
        
    Returns:
        Dictionary with cleanup results
    """
    try:
        from datetime import datetime, timedelta
        
        # Get active sessions older than max_age_hours
        cutoff_time = datetime.utcnow() - timedelta(hours=max_age_hours)
        
        # Get all active sessions
        active_sessions = await postgres.get_active_sessions(limit=100)
        
        zombie_sessions = []
        for session in active_sessions:
            start_time = session.get('start_time')
            if start_time and start_time < cutoff_time:
                zombie_sessions.append(session)
        
        if dry_run:
            return {
                "dry_run": True,
                "zombie_sessions_found": len(zombie_sessions),
                "zombie_sessions": [s['session_id'] for s in zombie_sessions],
                "cutoff_time": cutoff_time.isoformat()
            }
        
        # End zombie sessions
        ended_sessions = []
        for session in zombie_sessions:
            session_id = session['session_id']
            logger.info(f"üßπ Ending zombie session: {session_id}")
            
            # Mark session as ended
            await postgres.end_session(session_id, reason="zombie_cleanup")
            ended_sessions.append(session_id)
        
        logger.info(f"üßπ Cleaned up {len(ended_sessions)} zombie sessions")
        
        return {
            "success": True,
            "zombie_sessions_found": len(zombie_sessions),
            "zombie_sessions_ended": len(ended_sessions),
            "ended_sessions": ended_sessions,
            "cutoff_time": cutoff_time.isoformat()
        }
        
    except Exception as e:
        logger.error(f"Zombie session cleanup failed: {e}")
        return {
            "success": False,
            "error": str(e),
            "zombie_sessions_ended": 0
        }

async def export_session_context(session_id: str, session_context: Dict[str, Any]) -> Dict[str, Any]:
    """Export session context as a searchable document"""
    try:
        # Create a comprehensive document from session context
        document_content = generate_session_document(session_id, session_context)
        
        # Store as a searchable document (using existing document storage method)
        document_id = f"session_{session_id}_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}"
        
        # For now, store in Redis as cache - in production this would go to proper document storage
        await redis.setex(
            f"session_document:{document_id}",
            86400,  # 24 hours
            json.dumps({
                "document_id": document_id,
                "title": f"Session Context: {session_id}",
                "content": document_content,
                "project": "finderskeepers-v2",
                "tags": ["session_context", "conversation_history", "agent_actions"],
                "metadata": {
                    "session_id": session_id,
                    "document_type": "session_export",
                    "export_timestamp": datetime.utcnow().isoformat()
                }
            })
        )
        
        return {
            "success": True,
            "document_id": document_id,
            "content_length": len(document_content)
        }
    except Exception as e:
        logger.error(f"Failed to export session context: {e}")
        return {"success": False, "error": str(e)}

def generate_session_document(session_id: str, session_context: Dict[str, Any]) -> str:
    """Generate a comprehensive document from session context"""
    actions = session_context.get("actions", [])
    conversation = session_context.get("conversation_history", [])
    file_changes = session_context.get("file_changes", [])
    
    document_parts = [
        f"# Session Context: {session_id}",
        f"Generated: {datetime.utcnow().isoformat()}",
        "",
        "## Session Summary",
        f"- Total Actions: {len(actions)}",
        f"- Conversation Messages: {len(conversation)}",
        f"- Files Modified: {len(file_changes)}",
        f"- Success Rate: {session_context.get('summary', {}).get('success_rate', 0):.1%}",
        "",
        "## Recent Actions"
    ]
    
    for action in actions[:10]:  # Last 10 actions
        document_parts.append(f"- {action.get('timestamp', 'Unknown time')}: {action.get('description', 'No description')}")
        if not action.get('success', True):
            document_parts.append(f"  ‚ùå Failed: {action.get('details', {}).get('error_message', 'Unknown error')}")
    
    if file_changes:
        document_parts.extend([
            "",
            "## Files Modified",
            ""
        ])
        for file_change in file_changes[:20]:  # Last 20 file changes
            document_parts.append(f"- {file_change}")
    
    if conversation:
        document_parts.extend([
            "",
            "## Conversation Highlights",
            ""
        ])
        for msg in conversation[:10]:  # Last 10 messages
            msg_type = msg.get('message_type', 'unknown')
            content = msg.get('content', '')[:200] + '...' if len(msg.get('content', '')) > 200 else msg.get('content', '')
            document_parts.append(f"**{msg_type}**: {content}")
    
    return "\n".join(document_parts)

async def prepare_session_resume_info(session_id: str, session_context: Dict[str, Any], reason: str) -> Dict[str, Any]:
    """Prepare comprehensive resume information for the next session"""
    actions = session_context.get("actions", [])
    
    resume_info = {
        "previous_session_id": session_id,
        "end_reason": reason,
        "ended_at": datetime.utcnow().isoformat(),
        "session_stats": {
            "total_actions": len(actions),
            "success_rate": session_context.get('summary', {}).get('success_rate', 0),
            "conversation_messages": len(session_context.get('conversation_history', [])),
            "files_modified": len(session_context.get('file_changes', []))
        },
        "continuation_context": {
            "last_actions": actions[:5] if actions else [],
            "recent_files": session_context.get('file_changes', [])[:10],
            "unresolved_issues": [a for a in actions if not a.get('success', True)][:3],
            "key_achievements": [a for a in actions if a.get('success', True)][:3]
        },
        "next_session_preparation": {
            "recommended_actions": generate_intelligent_next_steps(session_context, actions),
            "priority_items": identify_priority_items(actions, session_context.get('file_changes', [])),
            "warnings": identify_session_warnings(actions, [a for a in actions if not a.get('success', True)])
        }
    }
    
    return resume_info

async def perform_session_cleanup(session_id: str) -> Dict[str, Any]:
    """Perform final cleanup operations for the session"""
    try:
        # Clear any session-specific cache entries
        await redis.delete(f"session_cache:{session_id}")
        await redis.delete(f"pending_operations:{session_id}")
        
        # Log cleanup completion
        logger.info(f"‚úÖ Session cleanup completed for {session_id}")
        
        return {"success": True, "message": "Session cleanup completed"}
    except Exception as e:
        logger.error(f"Session cleanup failed: {e}")
        return {"success": False, "error": str(e)}

@mcp.tool()
async def log_claude_action(
    session_id: str,
    action_type: str,
    description: str,
    files_affected: Optional[List[str]] = None,
    details: Optional[Dict[str, Any]] = None,
    success: bool = True,
    error_message: Optional[str] = None
) -> Dict[str, Any]:
    """
    Log a Claude Code action for automatic session tracking.
    
    Args:
        session_id: Session ID from initialize_claude_session
        action_type: Type of action (file_edit, command, config_change, etc.)
        description: Human-readable description of the action
        files_affected: List of files modified (optional)
        details: Action-specific details (optional)
        success: Whether action completed successfully (default: True)
        error_message: Error message if action failed (optional)
    
    Returns:
        Action logging confirmation with action_id
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("log_claude_action", {
        "session_id": session_id, "action_type": action_type, "description": description,
        "files_affected": files_affected, "details": details, "success": success, "error_message": error_message
    })
    
    try:
        logger.info(f"üìù Logging Claude action: {action_type} for session {session_id}")
        
        action_data = {
            "session_id": session_id,
            "action_type": action_type,
            "description": description,
            "details": details or {},
            "files_affected": files_affected or [],
            "success": success
        }
        
        if error_message:
            action_data["details"]["error_message"] = error_message
        
        # Generate action ID first
        action_id = f"action_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}_{uuid.uuid4().hex[:8]}"
        
        # Store directly to database first (critical for data persistence)
        try:
            await postgres.create_action(
                action_id=action_id,
                session_id=session_id,
                action_type=action_type,
                description=description,
                details=details or {},
                files_affected=files_affected or [],
                success=success
            )
            logger.info(f"‚úÖ Action {action_id} stored to database")
        except Exception as db_error:
            logger.error(f"Database storage failed for action {action_id}: {db_error}")
        
        # Call n8n Agent Action Tracker webhook (secondary)
        import httpx
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{activity_logger.n8n_webhook_url}/webhook/agent-actions",
                    json={**action_data, "action_id": action_id},
                    timeout=10.0
                )
                
                if response.status_code == 200:
                    logger.info(f"‚úÖ Action {action_id} sent to n8n webhook")
                else:
                    logger.warning(f"n8n webhook failed for action {action_id}: {response.status_code}")
        except Exception as webhook_error:
            logger.warning(f"n8n webhook error for action {action_id}: {webhook_error}")
        
        return {
            "action_id": action_id,
            "session_id": session_id,
            "status": "logged",
            "action_type": action_type,
            "success": success,
            "message": f"‚úÖ Action '{action_type}' logged successfully",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Claude action logging failed: {e}")
        return {
            "error": str(e),
            "status": "failed",
            "session_id": session_id,
            "action_type": action_type,
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def resume_session(
    project: str = "finderskeepers-v2",
    quick_summary: bool = True,
    auto_initialize: bool = False
) -> Dict[str, Any]:
    """
    üöÄ Smart session resumption with automatic context loading and continuation recommendations.
    
    This is the MAIN tool for starting your Claude Code sessions. It automatically:
    - Finds your most recent session
    - Loads full context from previous work
    - Analyzes what you were working on
    - Provides intelligent next-step recommendations
    - Initializes a new session with complete continuity
    
    Args:
        project: Project to resume (default: finderskeepers-v2)
        quick_summary: If True, show concise summary. If False, full detailed context
        auto_initialize: Whether to automatically start a new session (default: True)
    
    Returns:
        Complete session resume information with context, recommendations, and new session ID
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("resume_session", {
        "project": project, "quick_summary": quick_summary, "auto_initialize": auto_initialize
    })
    
    try:
        logger.info(f"üîÑ Resuming session for project: {project}")
        
        # Step 1: Check for active sessions first - CRITICAL FIX
        active_sessions = await postgres.get_active_sessions(project=project, limit=1)
        
        if active_sessions:
            active_session = active_sessions[0]
            logger.info(f"üîÑ Found active session: {active_session['session_id']}")
            
            # Return information about the active session instead of creating a new one
            return {
                "status": "active_session_found",
                "message": f"üìç Active session {active_session['session_id'][-8:]} already running. Continuing with existing session.",
                "active_session": active_session,
                "recommendations": ["üéØ Continue current work", "üìã Check recent actions", "üîç Review session progress"],
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Step 2: Try Redis cache first for instant results if no active session
        cached_resume_info = await redis.get(f"session_resume:{project}")
        
        if cached_resume_info:
            cached_data = json.loads(cached_resume_info)
            logger.info("‚ö° Using cached resume information from endsession")
            
            # Only initialize new session if requested AND no active session
            if auto_initialize:
                new_session_result = await initialize_claude_session(project=project)
                cached_data["new_session"] = new_session_result
                
            # Add instructions for full context if needed
            if quick_summary:
                cached_data["full_context_available"] = True
                cached_data["full_context_command"] = f"get_full_conversation_context(session_id='{cached_data.get('previous_session_id')}')"
                
            return {
                "status": "session_resumed",
                "message": f"üìã Resuming from session {cached_data.get('previous_session_id', 'unknown')[-8:]}",
                **cached_data,
                "timestamp": datetime.utcnow().isoformat()
            }
        
        # Step 3: Get latest completed session from database
        recent_completed_sessions = await postgres.get_recent_completed_sessions(project=project, limit=1)
        latest_session = recent_completed_sessions[0] if recent_completed_sessions else None
        
        if not latest_session:
            logger.info("üå± No previous session found, starting fresh")
            
            if auto_initialize:
                new_session_result = await initialize_claude_session(project=project)
                return {
                    "status": "fresh_start",
                    "message": "üå± No previous session found. Starting fresh with full context!",
                    "new_session": new_session_result,
                    "recommendations": ["üéØ Check project roadmap", "üìã Review recent commits", "üîç Search knowledge base for context"],
                    "timestamp": datetime.utcnow().isoformat()
                }
            else:
                return {"message": "No previous session found. Ready to start fresh!"}
        
        # Step 4: Basic resume info without expensive processing
        session_id = latest_session["session_id"]
        
        # Get minimal session data for fallback
        actions = await postgres.get_session_actions(session_id, limit=5)
        conversation_count = await postgres.get_conversation_message_count(session_id)
        
        resume_data = {
            "status": "session_resumed",
            "message": f"üìã Resuming from session {session_id[-8:]} (no cached summary)",
            "previous_session_id": session_id,
            "ended_at": latest_session.get("end_time"),
            "session_stats": {
                "total_actions": len(actions),
                "conversation_messages": conversation_count,
                "success_rate": 0  # Can't calculate without full processing
            },
            "continuation_context": {
                "last_actions": actions[:3],
                "recent_files": [],
                "unresolved_issues": [],
                "key_achievements": []
            },
            "next_session_preparation": {
                "recommended_actions": ["üîç Use get_full_conversation_context for detailed history"],
                "priority_items": ["üìã Review recent actions", "üîç Check file changes"],
                "warnings": ["‚ö†Ô∏è Session ended without proper cleanup - summary may be incomplete"]
            },
            "full_context_available": True,
            "full_context_command": f"get_full_conversation_context(session_id='{session_id}')",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Step 5: Initialize new session if requested
        if auto_initialize:
            new_session_result = await initialize_claude_session(project=project)
            resume_data["new_session"] = new_session_result
        
        return resume_data
        
    except Exception as e:
        logger.error(f"Session resume failed: {e}")
        return {
            "error": str(e),
            "status": "resume_failed",
            "message": "‚ùå Session resume failed. Starting fresh session instead.",
            "fallback_session": await initialize_claude_session(project=project) if auto_initialize else None,
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def endsession(
    session_id: Optional[str] = None,
    reason: str = "work_complete",
    completion_timeout: int = 30,
    prepare_resume: bool = True,
    export_context: bool = True
) -> Dict[str, Any]:
    """
    üèÅ Gracefully end your Claude Code session with guaranteed data persistence and resume preparation.
    
    This is the MAIN tool for ending your sessions properly. It ensures:
    - All conversation data is fully ingested and stored
    - Session context is exported for future searchability
    - Resume information is prepared for next session
    - Databases are properly updated before shutdown
    - Graceful termination with cleanup verification
    
    Args:
        session_id: Session ID to end (auto-detected if not provided)
        reason: Reason for ending session (default: work_complete)
        completion_timeout: Max seconds to wait for data ingestion (default: 30)
        prepare_resume: Whether to prepare resume info for next session (default: True)
        export_context: Whether to export session context as searchable document (default: True)
    
    Returns:
        Session termination confirmation with data persistence status
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("endsession", {
        "session_id": session_id, "reason": reason, "completion_timeout": completion_timeout,
        "prepare_resume": prepare_resume, "export_context": export_context
    })
    
    try:
        logger.info(f"üèÅ Ending session gracefully: {session_id or 'auto-detect'}")
        
        # Step 1: Auto-detect session ID if not provided
        if not session_id:
            active_sessions = await postgres.get_active_sessions(project="finderskeepers-v2", limit=1)
            if active_sessions:
                session_id = active_sessions[0]["session_id"]
            else:
                return {
                    "error": "No active session found to end",
                    "status": "no_active_session",
                    "timestamp": datetime.utcnow().isoformat()
                }
                
        # Set the session_id in the activity logger for proper shutdown
        activity_logger.set_session_id(session_id)
        
        # Step 2: Get full session context before termination
        session_context = await get_full_conversation_context(
            session_id=session_id,
            include_conversation_history=True,
            include_files=True,
            conversation_limit=100
        )
        
        # Ensure session_context has all expected keys to prevent NoneType errors
        if session_context is None:
            session_context = {}
            
        # Provide default values for missing keys
        session_context.setdefault("actions", [])
        session_context.setdefault("conversation_history", [])
        session_context.setdefault("file_changes", [])
        session_context.setdefault("summary", {})
        
        # Step 3: Wait for any pending data ingestion to complete
        logger.info(f"‚è≥ Waiting up to {completion_timeout}s for data ingestion to complete...")
        await asyncio.sleep(2)  # Initial buffer
        
        # Check for pending operations
        pending_operations = await check_pending_operations(session_id)
        wait_time = 0
        
        while pending_operations and wait_time < completion_timeout:
            await asyncio.sleep(1)
            wait_time += 1
            pending_operations = await check_pending_operations(session_id)
            
        if pending_operations:
            logger.warning(f"‚ö†Ô∏è Some operations still pending after {completion_timeout}s timeout")
        
        # Step 4: Export session context as searchable document
        export_result = None
        if export_context:
            export_result = await export_session_context(session_id, session_context)
        
        # Step 5: Prepare resume information for next session
        resume_info = None
        if prepare_resume:
            resume_info = await prepare_session_resume_info(session_id, session_context, reason)
            
            # Save to Redis with 24-hour expiration
            await redis.setex(
                f"session_resume:finderskeepers-v2", 
                86400, 
                json.dumps(resume_info)
            )
        
        # Step 6: Notify n8n about session end first
        session_end_data = {
            "session_id": session_id,
            "status": "ended",
            "end_time": datetime.utcnow().isoformat(),
            "reason": reason
        }
        
        # Call n8n Session End webhook
        import httpx
        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{activity_logger.n8n_webhook_url}/webhook/session-end",
                    json=session_end_data,
                    timeout=10.0
                )
                
                if response.status_code == 200:
                    logger.info(f"‚úÖ Session end notification sent to n8n for session {session_id}")
                else:
                    logger.warning(f"n8n session-end webhook failed: {response.status_code}")
        except Exception as webhook_error:
            logger.warning(f"n8n session-end webhook error: {webhook_error}")
        
        # Step 7: Mark session as ended in database (using existing activity logger)
        end_result = await activity_logger.shutdown(reason)
        
        # Ensure end_result is not None to prevent NoneType errors
        if end_result is None:
            end_result = {"success": False, "error": "activity_logger.shutdown returned None"}
        
        # Step 8: Log the session termination
        await activity_logger.log_action(
            action_type="session_end",
            description=f"Session {session_id} ended gracefully",
            details={
                "session_id": session_id,
                "reason": reason,
                "completion_timeout": completion_timeout,
                "data_exported": export_context,
                "resume_prepared": prepare_resume,
                "pending_operations_cleared": not pending_operations,
                "actions_processed": len(session_context.get("actions", [])),
                "conversation_messages": len(session_context.get("conversation_history", []))
            }
        )
        
        # Step 9: Final cleanup and verification
        cleanup_result = await perform_session_cleanup(session_id)
        
        # Ensure cleanup_result is not None to prevent NoneType errors
        if cleanup_result is None:
            cleanup_result = {"success": False, "error": "perform_session_cleanup returned None"}
        
        return {
            "status": "session_ended",
            "session_id": session_id,
            "reason": reason,
            "data_persistence": {
                "context_exported": export_result is not None,
                "resume_prepared": resume_info is not None,
                "database_updated": end_result.get("success", False),
                "cleanup_completed": cleanup_result.get("success", False)
            },
            "session_summary": {
                "total_actions": len(session_context.get("actions", [])),
                "total_messages": len(session_context.get("conversation_history", [])),
                "files_modified": len(session_context.get("file_changes", [])),
                "success_rate": session_context.get("summary", {}).get("success_rate", 0)
            },
            "next_session_ready": resume_info is not None,
            "message": f"‚úÖ Session {session_id[-8:]} ended gracefully. All data preserved for next session.",
            "timestamp": datetime.utcnow().isoformat()
        }
        
    except Exception as e:
        logger.error(f"Session termination failed: {e}")
        return {
            "error": str(e),
            "status": "termination_failed",
            "session_id": session_id,
            "message": "‚ùå Session termination encountered errors. Some data may not be preserved.",
            "timestamp": datetime.utcnow().isoformat()
        }

@mcp.tool()
async def cleanup_zombie_sessions_tool(
    max_age_hours: int = 6,
    dry_run: bool = True
) -> Dict[str, Any]:
    """
    Clean up zombie sessions that have been active for too long
    
    Args:
        max_age_hours: Maximum age in hours before a session is considered zombie (default: 6)
        dry_run: If True, only identify zombie sessions without ending them (default: True)
        
    Returns:
        Dictionary with cleanup results
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("cleanup_zombie_sessions_tool", {
        "max_age_hours": max_age_hours, "dry_run": dry_run
    })
    
    logger.info(f"üßπ {'Checking for' if dry_run else 'Cleaning up'} zombie sessions older than {max_age_hours} hours")
    result = await cleanup_zombie_sessions(max_age_hours, dry_run)
    
    # Log the result through universal middleware
    await _log_tool_call("cleanup_zombie_sessions_tool", {}, result)
    
    return result

@mcp.tool()
async def get_project_overview(
    project: str,
    include_recent_activity: bool = True,
    activity_days: int = 7
) -> Dict[str, Any]:
    """
    Get comprehensive overview of a project including documents, sessions, and activity.
    
    Args:
        project: Project name to analyze
        include_recent_activity: Whether to include recent session activity (default: True)
        activity_days: Number of days for recent activity (default: 7)
    
    Returns:
        Complete project overview with statistics and recent activity
    """
    # Universal conversation logging for this tool call
    await _log_tool_call("get_project_overview", {
        "project": project, "include_recent_activity": include_recent_activity, "activity_days": activity_days
    })
    
    try:
        logger.info(f"üìä Getting project overview: {project}")
        
        # Get project statistics
        project_stats = await postgres.get_project_statistics(project)
        
        # Get recent documents
        recent_docs = await postgres.get_recent_documents(project, limit=10)
        
        # Get recent activity if requested
        recent_activity = []
        if include_recent_activity:
            recent_activity = await postgres.get_recent_project_activity(
                project, 
                days=activity_days,
                limit=20
            )
        
        # Get knowledge graph entities for this project
        project_entities = await neo4j.get_project_entities(project, limit=15)
        
        result = {
            "project": project,
            "statistics": project_stats,
            "recent_documents": recent_docs,
            "recent_activity": recent_activity if include_recent_activity else [],
            "knowledge_entities": project_entities,
            "overview": {
                "total_documents": project_stats.get("total_documents", 0),
                "total_sessions": project_stats.get("total_sessions", 0),
                "total_actions": project_stats.get("total_actions", 0),
                "activity_level": "high" if len(recent_activity) > 10 else "medium" if len(recent_activity) > 5 else "low"
            },
            "analysis_period": f"Last {activity_days} days",
            "timestamp": datetime.utcnow().isoformat()
        }
        
        # Log the successful result through universal middleware
        await _log_tool_call("get_project_overview", {}, result)
        
        return result
        
    except Exception as e:
        logger.error(f"Project overview failed: {e}")
        return {
            "error": str(e),
            "project": project,
            "statistics": {},
            "timestamp": datetime.utcnow().isoformat()
        }

# ========================================
# RESOURCES - Readable data sources
# ========================================

@mcp.resource("schema://database")
async def get_database_schema() -> str:
    """Provide the PostgreSQL database schema as a resource"""
    try:
        # Log resource access
        await activity_logger.log_resource_access("schema://database")
        
        schema_info = await postgres.get_detailed_schema()
        
        return f"""# FindersKeepers v2 Database Schema

## Core Tables

### Agent Session Tracking
- **agent_sessions**: Session metadata, context, timing
- **agent_actions**: Individual actions within sessions
- **config_changes**: Configuration change tracking

### Document Management  
- **documents**: Document metadata, project association
- **document_chunks**: Text chunks for vector search
- **knowledge_entities**: Graph entity references

## Vector & Graph Storage
- **Qdrant**: Vector embeddings for semantic search
- **Neo4j**: Knowledge graph with entities and relationships
- **Redis**: Session caching and real-time data

## Schema Details
{json.dumps(schema_info, indent=2)}

*Generated at: {datetime.utcnow().isoformat()}*
"""
    except Exception as e:
        return f"Error retrieving schema: {e}"

@mcp.resource("stats://knowledge-base")
async def get_knowledge_stats() -> str:
    """Provide comprehensive knowledge base statistics and health metrics"""
    try:
        # Log resource access
        await activity_logger.log_resource_access("stats://knowledge-base")
        
        # Get stats from all data sources
        postgres_stats = await postgres.get_comprehensive_stats()
        neo4j_stats = await neo4j.get_graph_stats()
        qdrant_stats = await qdrant.get_collection_stats()
        
        return f"""# FindersKeepers v2 Knowledge Base Statistics

## Document Storage (PostgreSQL)
- üìÑ **Total Documents**: {postgres_stats.get('total_documents', 0):,}
- üß© **Total Chunks**: {postgres_stats.get('total_chunks', 0):,}
- üìä **Total Sessions**: {postgres_stats.get('total_sessions', 0):,}
- ‚ö° **Total Actions**: {postgres_stats.get('total_actions', 0):,}
- üìÅ **Active Projects**: {postgres_stats.get('active_projects', 0)}

## Knowledge Graph (Neo4j)
- üîó **Total Nodes**: {neo4j_stats.get('total_nodes', 0):,}
- üåê **Total Relationships**: {neo4j_stats.get('total_relationships', 0):,}
- üè∑Ô∏è **Node Types**: {', '.join(neo4j_stats.get('node_types', []))}
- üìà **Graph Density**: {neo4j_stats.get('density', 0):.3f}

## Vector Search (Qdrant)
- üóÇÔ∏è **Collections**: {len(qdrant_stats.get('collections', []))}
- üî¢ **Total Vectors**: {qdrant_stats.get('total_vectors', 0):,}
- üìê **Vector Dimensions**: {qdrant_stats.get('vector_size', 0)}
- üíæ **Index Type**: {qdrant_stats.get('index_type', 'HNSW')}

## Performance Metrics
- üöÄ **Avg Query Time**: {postgres_stats.get('avg_query_time', 0):.2f}ms
- üìä **Vector Search Speed**: {qdrant_stats.get('search_speed', 0)} queries/sec
- üß† **Graph Traversal**: {neo4j_stats.get('traversal_speed', 0)} nodes/sec

## Health Status
- ‚úÖ **PostgreSQL**: {'Healthy' if postgres_stats.get('healthy', False) else 'Issues'}
- ‚úÖ **Neo4j**: {'Healthy' if neo4j_stats.get('healthy', False) else 'Issues'}  
- ‚úÖ **Qdrant**: {'Healthy' if qdrant_stats.get('healthy', False) else 'Issues'}

*Last Updated: {datetime.utcnow().isoformat()}*
*Knowledge Server Version: 2.0.0*
"""
    except Exception as e:
        return f"Error retrieving knowledge stats: {e}"

@mcp.resource("guide://knowledge-search")
async def get_search_guide() -> str:
    """Provide a comprehensive guide for effective knowledge searching"""
    # Log resource access
    await activity_logger.log_resource_access("guide://knowledge-search")
    
    return """# FindersKeepers v2 Knowledge Search Guide

## Effective Search Strategies

### 1. Semantic Search
Use **natural language** to find conceptually related content:
- ‚ùì "How to configure Docker with GPU support"
- ‚ùì "FastAPI authentication patterns"
- ‚ùì "Database migration procedures"

### 2. Project-Specific Search
Filter by project context for focused results:
- üéØ project="bitcain" ‚Üí BitCoin-related documents only
- üéØ project="skellekey" ‚Üí SkelleKey project documentation
- üéØ project="finderskeepers" ‚Üí This project's documentation

### 3. Tag-Based Filtering
Use tags to narrow down document types:
- üè∑Ô∏è tags=["configuration"] ‚Üí Setup and config docs
- üè∑Ô∏è tags=["troubleshooting"] ‚Üí Problem-solving guides
- üè∑Ô∏è tags=["architecture"] ‚Üí System design documents

### 4. Knowledge Graph Queries
Explore relationships and connections:
- üß† "What entities are connected to user authentication?"
- üß† "Show relationships between Docker and GPU configuration"
- üß† "Find all procedures linked to database migrations"

### 5. Session Context Analysis
Understand agent activity and history:
- üìã Recent sessions by agent type or project
- üìã File changes and modifications tracking
- üìã Success patterns and common issues

## Search Quality Tips

### Improve Results
- ‚úÖ Use **specific technical terms** when searching for code
- ‚úÖ Include **context** like "FastAPI" + "authentication" 
- ‚úÖ Try **variations** of your query if results aren't helpful
- ‚úÖ Use **filters** to narrow scope (project, tags, date ranges)

### Common Patterns
- üîç **"How to..."** ‚Üí Procedural documentation
- üîç **"Error: [message]"** ‚Üí Troubleshooting guides
- üîç **"[Technology] + [Feature]"** ‚Üí Implementation examples
- üîç **"Configuration + [Service]"** ‚Üí Setup documentation

### Advanced Techniques
- üéØ **Similarity Analysis**: Find documents similar to one you know is relevant
- üéØ **Project Overview**: Get comprehensive project status and activity
- üéØ **Cross-Reference**: Use knowledge graph to find unexpected connections

## Example Queries

### Document Search Examples
```
search_documents(
  query="Docker GPU configuration with RTX 2080 Ti",
  project="finderskeepers",
  limit=5,
  min_score=0.6
)
```

### Knowledge Graph Examples  
```
query_knowledge_graph(
  question="What are the relationships between FastAPI and database connections?",
  max_depth=2,
  limit=15
)
```

### Session Analysis Examples
```
get_session_context(
  agent_type="claude",
  project="finderskeepers", 
  recent_actions=10,
  include_files=true
)
```

**Remember**: The knowledge base grows with every document you ingest, so search strategies that don't work today might work perfectly tomorrow!
"""

# ========================================
# PROMPTS - Guided interactions
# ========================================

@mcp.prompt()
async def knowledge_search_prompt() -> Prompt:
    """Provide a guided prompt template for knowledge searching"""
    return Prompt(
        name="knowledge_search",
        description="Guided knowledge base search with intelligent context",
        messages=[
            {
                "role": "user", 
                "content": """I need to search the FindersKeepers v2 knowledge base intelligently. Please help me find the most relevant information.

## What I'm Looking For:
**Search Query**: {query}

## Context Filters:
- **Project Focus**: {project}
- **Document Types**: {doc_types}
- **Time Period**: {time_period}
- **Minimum Relevance**: {min_score}

## Search Strategy:
1. **Semantic Search**: Find conceptually related content using AI embeddings
2. **Knowledge Graph**: Explore entity relationships and connections  
3. **Session History**: Check if agents have worked on this before
4. **Similarity Analysis**: Find documents similar to known relevant ones

## Additional Context:
{additional_context}

Please execute a comprehensive search across all knowledge sources and provide the most relevant results with explanations of why each result might be helpful.
"""
            }
        ],
        arguments=[
            {"name": "query", "description": "Main search query or question", "required": True},
            {"name": "project", "description": "Project context filter", "required": False},
            {"name": "doc_types", "description": "Comma-separated document types", "required": False},
            {"name": "time_period", "description": "Time period for search", "required": False},
            {"name": "min_score", "description": "Minimum relevance score (0.0-1.0)", "required": False},
            {"name": "additional_context", "description": "Any additional context or constraints", "required": False}
        ]
    )

# ========================================
# SERVER LIFECYCLE
# ========================================

async def startup():
    """Initialize database connections and verify system health"""
    logger.info("üîç Starting FindersKeepers Knowledge MCP Server...")
    
    try:
        # Initialize activity logger for n8n workflow integration
        await activity_logger.initialize()
        
        # Initialize conversation monitoring for exit command detection
        global conversation_monitor
        conversation_monitor = await setup_conversation_monitoring(activity_logger)
        
        # Initialize universal conversation logging middleware
        middleware = initialize_conversation_middleware(activity_logger)
        logger.info("üåê Universal conversation logging middleware initialized")
        
        # Start parent process monitoring
        global parent_process_monitor_task
        parent_process_monitor_task = asyncio.create_task(monitor_parent_process())
        
        # Initialize database connections
        await postgres.connect()
        logger.info("‚úÖ PostgreSQL connected")
        
        await neo4j.connect()
        logger.info("‚úÖ Neo4j connected")
        
        await qdrant.connect()
        logger.info("‚úÖ Qdrant connected")
        
        await redis.connect()
        logger.info("‚úÖ Redis connected")
        
        # Verify system health
        health_check = await perform_health_check()
        if health_check["all_healthy"]:
            logger.info("üöÄ Knowledge server ready for agent queries!")
            await activity_logger.log_action(
                action_type="server_startup",
                description="MCP Knowledge Server started successfully",
                details=health_check
            )
        else:
            logger.warning(f"‚ö†Ô∏è Some services have issues: {health_check}")
            
    except Exception as e:
        logger.error(f"‚ùå Failed to start knowledge server: {e}")
        raise

async def shutdown():
    """Gracefully close all database connections"""
    logger.info("üîå Shutting down FindersKeepers Knowledge MCP Server...")
    
    try:
        # Stop conversation monitoring
        global conversation_monitor
        if conversation_monitor:
            conversation_monitor.stop_monitoring()
            
        # Stop parent process monitoring
        global parent_process_monitor_task
        if parent_process_monitor_task:
            parent_process_monitor_task.cancel()
            try:
                await parent_process_monitor_task
            except asyncio.CancelledError:
                pass
            
        # Log shutdown to n8n
        await activity_logger.shutdown("graceful_shutdown")
        
        await postgres.disconnect()
        await neo4j.disconnect()
        await qdrant.disconnect()
        await redis.disconnect()
        
        logger.info("üëã Knowledge server shutdown complete")
        
    except Exception as e:
        logger.error(f"‚ùå Error during shutdown: {e}")

async def perform_health_check() -> Dict[str, Any]:
    """Perform comprehensive health check of all services"""
    health = {
        "postgres": await postgres.health_check(),
        "neo4j": await neo4j.health_check(),
        "qdrant": await qdrant.health_check(),
        "redis": await redis.health_check()
    }
    
    health["all_healthy"] = all(health.values())
    health["timestamp"] = datetime.utcnow().isoformat()
    
    return health

async def monitor_parent_process():
    """Monitor parent process and shut down if it terminates"""
    parent_pid = os.getppid()
    logger.info(f"üëÅÔ∏è Monitoring parent process PID: {parent_pid}")
    
    # Fixed: Use event-driven parent monitoring instead of infinite loop
    try:
        # Set up signal handlers for proper shutdown
        def handle_shutdown():
            logger.info("üö™ Parent process terminated, shutting down MCP server")
            loop = asyncio.get_event_loop()
            loop.create_task(shutdown_gracefully())
        
        async def shutdown_gracefully():
            try:
                await activity_logger.shutdown("parent_process_terminated")
                
                # Stop conversation monitoring
                global conversation_monitor
                if conversation_monitor:
                    conversation_monitor.stop_monitoring()
                
                # Disconnect databases gracefully
                try:
                    await postgres.disconnect()
                    await neo4j.disconnect()
                    await qdrant.disconnect()
                    await redis.disconnect()
                except Exception as db_error:
                    logger.error(f"Database disconnect error: {db_error}")
                
                logger.info("üëã MCP server shutdown complete (parent terminated)")
            except Exception as e:
                logger.error(f"Shutdown error: {e}")
        
        # Check parent exists once, then rely on signals
        if not psutil.pid_exists(parent_pid):
            logger.info("üö™ Parent process not found at startup")
            await shutdown_gracefully()
            return
        
        logger.info("‚úÖ Parent process monitoring enabled (signal-based)")
        
    except asyncio.CancelledError:
        logger.info("üëÅÔ∏è Parent process monitoring cancelled")

if __name__ == "__main__":
    # Load environment variables from .env file
    load_dotenv()
    
    # Run the MCP server
    logger.info("üåü FindersKeepers v2 Knowledge MCP Server")
    logger.info("üîó Connecting AI agents to universal knowledge...")
    
    import signal
    import sys
    
    def signal_handler(signum, frame):
        """Handle termination signals gracefully"""
        signal_name = "SIGTERM" if signum == signal.SIGTERM else "SIGINT" if signum == signal.SIGINT else f"SIG{signum}"
        logger.info(f"üõë Received {signal_name} ({signum}), shutting down gracefully...")
        # Run shutdown in a new event loop since we might be in a signal handler
        try:
            import asyncio
            # Create async shutdown function with signal reason
            async def signal_shutdown():
                global conversation_monitor
                if conversation_monitor:
                    conversation_monitor.stop_monitoring()
                await activity_logger.shutdown("signal_termination")
                await postgres.disconnect()
                await neo4j.disconnect()
                await qdrant.disconnect()
                await redis.disconnect()
                logger.info("üëã Knowledge server shutdown complete (signal)")
            
            asyncio.run(signal_shutdown())
        except Exception as e:
            logger.error(f"Error during signal shutdown: {e}")
        finally:
            sys.exit(0)
    
    # Register signal handlers for graceful shutdown
    signal.signal(signal.SIGTERM, signal_handler)
    signal.signal(signal.SIGINT, signal_handler)
    
    # Run startup before starting the server
    import asyncio
    asyncio.run(startup())
    
    try:
        mcp.run()
    except KeyboardInterrupt:
        logger.info("üõë KeyboardInterrupt received")
    except Exception as e:
        logger.error(f"‚ùå Server error: {e}")
    finally:
        # Run shutdown on exit
        try:
            asyncio.run(shutdown())
        except Exception as e:
            logger.error(f"Error during final shutdown: {e}")